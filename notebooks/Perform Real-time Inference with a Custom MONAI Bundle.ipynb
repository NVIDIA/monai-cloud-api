{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to do Real-Time Inference Using a Custom MONAI Bundle with NVIDIA Cloud APIs\n",
    "\n",
    "In this guide, we will walk you through setting up a real-time inference system using a custom MONAI bundle with MONAI Cloud APIs. We will cover experiment setup, making predictions on-the-fly, and managing outputs to ensure a seamless, efficient, and real-time decision-making pipeline.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Perform%20Real-time%20Inference%20with%20a%20Custom%20MONAI%20Bundle.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Environment Setup\n",
    "- Dataset Creation\n",
    "- Custom MONAI Bundle Creation\n",
    "- Configuring Experiment to Enable the Real-Time Inference\n",
    "- Cache Image for the Inference Request\n",
    "- Triggering Inference on a Specified Image\n",
    "- Stopping the Experiment from Real-Time Inference Mode\n",
    "- Cleaning up\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Transitioning to real-time inference can substantially elevate the responsiveness and applicability of AI models in healthcare. Analyzing and interpreting medical images as they are generated, and instantly providing insights, can be transformative, offering benefits such as improved patient outcomes and more efficient use of medical resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import requests\" || pip install -q \"requests\"\n",
    "!python -c \"import requests_toolbelt\" || pip install -q \"requests_toolbelt\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from requests_toolbelt.multipart.decoder import MultipartDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "# Remote storage object info\n",
    "access_id = \"<user id>\"\n",
    "access_secret = \"<storage secret>\"\n",
    "container_url = \"<remote storage object address>\"\n",
    "inference_image_id = \"<inference image id>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", json={\"ngc_api_key\": ngc_api_key})\n",
    "response.raise_for_status()\n",
    "assert \"user_id\" in response.json(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json(), \"token is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "ngc_org = \"iasixjqzw1hj\"  # This is the default org for MONAI users. Please select the correct org if you are not using the default one.\n",
    "base_url = f\"{api_url}/orgs/{ngc_org}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "print(\"API Calls will be forwarded to\", base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "### **1. Remote Storage Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports a range of other cloud storage solutions, including Azure Blob Storage, Google Cloud Storage (GCP) and Amazon S3, providing you with the flexibility to choose the service that best fits your project's needs. Below is an example of Azure:\n",
    "\n",
    "**Steps:**\n",
    "1. Creating a Storage Account and Container\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. Container URL\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "### **2. Obtaining Credentials**\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the 'Access keys' section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "### **3. Creating a Manifest JSON File**\n",
    "\n",
    "In the root of your Azure container, create a manifest JSON file to keep track of your datasets. The file format is as follows:\n",
    "\n",
    "For a segmentation task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]/[subfolder-path]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"path\": [\"path/to/your/label_1\"],\n",
    "                \"id\": \"unique-uuid-2\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "For a non-segmentation task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]\",\n",
    "    \"label_key\": [\"bbox\", \"label\"],\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"bbox\": ...,\n",
    "            \"label\": ...\n",
    "        }\n",
    "        // Additional data objects should follow the same format\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "- Each dataset (training, testing, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" and \"label\" fields\n",
    "- Each \"image\"/\"label\" field should contain \"path\" field, which is the list of relative path to the image/label files\n",
    "- Please provide the \"id\" field of the \"image\"/\"label\", if there is not one please provide a random uuid generated by `uuid` package\n",
    "- The `label_key` is optional, with a default of `[\"label\"]`\n",
    "\n",
    "After preparing your dataset, please modify the following variables in **Environment Setup**:\n",
    "\n",
    "```python\n",
    "access_id = ...\n",
    "access_secret = ...\n",
    "container_url = ...\n",
    "inference_image_id = ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Remote Storage Object to Create Datasets\n",
    "\n",
    "After you've completed the steps above, it's time to run the API to create your dataset.  Below you'll find an example request along with associated parameters and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"MONAI_CLOUD\",\n",
    "    \"description\":\"Remote storage object dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": container_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MONAI Bundle Creation\n",
    "\n",
    "1. **MONAI Bundle**: We're using the Endoscopic Inbody Classification MONAI bundle from the MONAI Model Zoo as an example. Users can build their own MONAI bundles fitting their applications.\n",
    "2. **Dataset Setup**: All data is under one dataset ID for this demo. Adjust as per your data structure.\n",
    "3. **Pretrained Weights**: The Official MONAI bundles have pretrained weights.\n",
    "\n",
    "Here are some notes about the payload used to create the experiment:\n",
    "\n",
    "- name: A user-defined name for the training experiment, here named \"my_inbody_clf\".\n",
    "- description: A brief description of the experiment. Optional\n",
    "- network_arch: Specifies the architecture of the network. The value \"monai_custom\" indicates that a custom network architecture is being used. The user must provide the `bundle_url` with such custom architecture.\n",
    "- bundle_url: Indicating the specific location of the MONAI bundle to be used in this training experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Experiment to Enable the Real-time Inference\n",
    "\n",
    "**Note:** We're going to use the `realtime_infer` parameter when creating our experiment as that will automatically load the experiment and make sure it's ready for real-time inference workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/monaihosting/endoscopic_inbody_classification/versions/0.4.6/files/endoscopic_inbody_classification_v0.4.6.zip\"\n",
    "    \n",
    "data = {\n",
    "  \"name\": \"my_inbody_clf\",\n",
    "  \"description\": \"from MONAI model zoo\",\n",
    "  \"network_arch\": \"monai_custom\",\n",
    "  \"train_datasets\": [ dataset_id ],\n",
    "  \"eval_dataset\": dataset_id,\n",
    "  \"realtime_infer\": True,\n",
    "  \"bundle_url\": bundle_url,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "print(\"Experiment creation succeeded with experiment ID:\", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Image for the Inference Request\n",
    "\n",
    "The code sends a request to the \"cacheimage\" action. Users need to specify an image_id manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"action\": \"cacheimage\",\n",
    "    \"specs\": {\"image\": inference_image_id}\n",
    "}\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"cache image failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering Inference on a Specified Image\n",
    "\n",
    "Initiate an inference process on a particular image within an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipart_data = MultipartDecoder.from_response(response)\n",
    "for part in multipart_data.parts:\n",
    "    filename = part.headers[b\"Content-Disposition\"].decode().split(\";\")[1].split(\"=\")[1].strip('\"')\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(part.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping the Experiment from Real-Time Inference Mode\n",
    "\n",
    "When the experiment is created with `realtime_infer` as `True`, it will reserve one GPU to process the inference requests.\n",
    "\n",
    "After we have finished the inference process, we would like to release the GPU resource for other tasks.\n",
    "\n",
    "To achieve this, we can switch the `realtime_infer` from `True` to `False`.\n",
    "\n",
    "Note: this step is irreversible, which means you can't set the `realtime_infer` from `False` to `True`. To bootstrap another inference, you will have to create another experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"realtime_infer\": False,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.patch(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 200, f\"stop job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "Delete the experiment and dataset after jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showcase a streamlined approach to do real-time inference on a custom MONAI bundle, and processing within a NVIDIA MONAI Cloud API-driven system. This method ensures efficient operations, allowing users to focus on model refinement and analysis while the system efficiently manages image selection and inference tasks, demonstrating the transformative potential of integrating advanced AI in real-time decision-making workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
