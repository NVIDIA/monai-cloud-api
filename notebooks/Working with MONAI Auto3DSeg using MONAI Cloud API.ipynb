{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with MONAI Auto3DSeg using MONAI Cloud API\n",
    "This comprehensive guide is designed to help you navigate the process of training and testing with MONAI Auto3DSeg on the NVIDIA DGX Cloud, focusing on leveraging the powerful capabilities of DGX systems for medical imaging applications.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Datasets Creation\n",
    "- Auto3DSeg Experiment Creation\n",
    "- Monitoring Job Status\n",
    "- AutoML Generated Model Inference\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Auto3DSeg is a MONAI native project, tailored to demonstrate optimal 3D segmentation workflows for various algorithms. It simplifies the process for non-experts, allowing them to train models on 3D CT or MRI data with just a few lines of code. For experts, it offers a compilation of best practices for segmentation training using MONAI components. This enables users to achieve and customize state-of-the-art baseline segmentation performances.\n",
    "\n",
    "A key focus of Auto3DSeg is on computational efficiency, aiming to minimize training and inference times while maximizing GPU compute utilization. Leveraging the MONAI Cloud API enhances this efficiency, streamlining data management and model training. Integrated with NVIDIA DGX Cloud, it provides scalable computational resources, ideal for handling large medical imaging datasets and complex training scenarios. This combination accelerates the development of advanced medical imaging solutions.\n",
    "\n",
    "If you haven't already generated your key or if you're unsure about the process, follow our step-by-step guide for [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"<MONAI Cloud API URL>\"\n",
    "ngc_api_key = \"<NGC API Key>\"\n",
    "\n",
    "# Object Storage URL and Credentials\n",
    "train_data_url = \"<container url for the training dataset>\"      # Training dataset container URL\n",
    "inference_data_url = \"<container url for the inference dataset>\" # Inference dataset container URL\n",
    "storage_client_id = \"<object storage ID>\"                        # Object storage username/id\n",
    "storage_client_secret = \"<object storage secret>\"                # Object storage password/secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login into NGC and API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGC Login\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", data=json.dumps({\"ngc_api_key\": ngc_api_key}))\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "\n",
    "uid = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{api_url}/users/{uid}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Creation\n",
    "\n",
    "### **1. Remote Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports a range of other cloud storage solutions, including Azure Blob Storage, Google Cloud Storage (GCP) and Amazon S3, providing you with the flexibility to choose the service that best fits your project's needs. Below is an example of Azure:\n",
    "\n",
    "**Steps:**\n",
    "1. Creating a Storage Account and Container\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. Container URL\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "#### Obtaining Credentials\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the 'Access keys' section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "#### Creating a Manifest JSON File\n",
    "\n",
    "In the root of your Azure container, create a manifest JSON file to keep track of your datasets. The file format is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"path\": [\"path/to/your/label_1\"],\n",
    "                \"id\": \"unique-uuid-2\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "- Each dataset (training, testing, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" and \"label\" fields\n",
    "- Each \"image\"/\"label\" field should contain \"path\" field, which is the list of relative path to the image/label files\n",
    "\n",
    "\n",
    "After preparing your dataset, please modify the following variables in [Parameters](#Parameters):\n",
    "\n",
    "```python\n",
    "train_data_url = ...\n",
    "inference_data_url = ...\n",
    "storage_client_id = ...\n",
    "storage_client_secret = ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Creating the training datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = f\"{base_url}/datasets\"\n",
    "data = {\n",
    "    \"name\": \"train_sim_data_azure\",\n",
    "    \"description\": \"Simulated dataset for training Auto3DSeg on Azure\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": train_data_url,\n",
    "    \"client_id\": storage_client_id,\n",
    "    \"client_secret\": storage_client_secret,\n",
    "}\n",
    "response = requests.post(dataset_api, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "train_dataset_id = res[\"id\"]\n",
    "print(\"Train dataset created with dataset IDï¼š\", train_dataset_id)\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(json.dumps(res, indent=2))\n",
    "print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Creating the inference datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = f\"{base_url}/datasets\"\n",
    "data = {\n",
    "    \"name\": \"test_sim_data_azure\",\n",
    "    \"description\": \"Simulated for evaluation of Auto3DSeg on Azure\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": inference_data_url,\n",
    "    \"client_id\": storage_client_id,\n",
    "    \"client_secret\": storage_client_secret,\n",
    "}\n",
    "response = requests.post(dataset_api, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "infer_dataset_id = res[\"id\"]\n",
    "print(\"Inference dataset created with dataset ID:\", infer_dataset_id)\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(json.dumps(res, indent=2))\n",
    "print(\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto3DSeg Experiment Creation\n",
    "\n",
    "Users have the ability to initiate an experiment and execute the **auto3dseg** action to activate the Auto3DSeg pipeline. This process automatically sets up four distinct neural networks, undertaking multi-fold training to attain state-of-the-art performance in segmentation tasks. While the module is designed to be highly configurable to cater to various user needs, it maintains simplicity by requiring only minimal user input.\n",
    "\n",
    "Incorporating MONAI Cloud API into this workflow further enhances the user experience. The API facilitates seamless integration and management of data, models, and computational resources within a unified interface. This integration not only simplifies the process but also ensures efficient use of computational resources, particularly when running complex and resource-intensive tasks.\n",
    "\n",
    "**Minimal Inputs**\n",
    "\n",
    "Moreover, with the minimal input, users benefit from these advanced capabilities without needing to delve into complex configurations, making the Auto3DSeg pipeline accessible to a wide range of users, from beginners to experts in the field of medical imaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Find the base experiment for Auto3DSeg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "automl_base_exps = [p for p in res if p[\"network_arch\"] == \"monai_automl\" and p[\"name\"] == \"MONAI Auto3dSeg\"]\n",
    "assert len(automl_base_exps) > 0, \"No base experiment found for Auto3DSeg Experiment\"\n",
    "print(f\"List of available base experiments for Auto3DSeg:\")\n",
    "print({p[\"id\"]: {\"name\": p[\"name\"], \"version\": p[\"version\"]} for p in automl_base_exps})\n",
    "base_experiment = sorted(automl_base_exps, key=lambda x: x[\"version\"])[-1]  # take the latest version\n",
    "base_experiment_id = base_experiment[\"id\"]\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(f\"Base experiment ID for '{base_experiment['name']}' v{base_experiment['version']}: {base_experiment_id}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create MONAI AutoML Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"automl_experiment\",\n",
    "    \"description\": \"MONAI AutoML Experiment for Segmentation\",\n",
    "    \"type\": \"medical\",\n",
    "    \"base_experiment\": [base_experiment_id],\n",
    "    \"network_arch\": \"monai_automl\",\n",
    "    \"train_datasets\": [train_dataset_id],\n",
    "    \"inference_dataset\": infer_dataset_id,\n",
    "    \"realtime_infer\": False,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Experiment creation failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "automl_experiment_id = res[\"id\"]\n",
    "print(\"Experiment creation succeeded with experiment ID:\", automl_experiment_id)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "print(json.dumps(res, indent=2))\n",
    "print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Run Auto3DSeg Action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"action\": \"auto3dseg\",\n",
    "    \"specs\": {\n",
    "        \"num_gpu\": 2,\n",
    "        \"output_experiment_name\": \"Auto3DSegGenModel\",\n",
    "        \"output_experiment_description\": \"AutoML generated segmentation experiment using MONAI Auto3DSeg\",\n",
    "        \"modality\": \"MRI\",\n",
    "        \"num_fold\": 1,\n",
    "        \"train_params\": {\n",
    "            \"num_epochs_per_validation\": 1,\n",
    "            \"num_images_per_batch\": 2,\n",
    "            \"num_epochs\": 1,\n",
    "            \"num_warmup_epochs\": 1,\n",
    "            \"use_pretrain\": False,  # can modify to True to use pretrained weights\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{automl_experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create job failed, got {response.json()}.\"\n",
    "automl_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID:\", automl_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Management**\n",
    "\n",
    "User can track the experiments in Auto3DSeg by adding a mlflow tracking server URL to the payload data:\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    ...,\n",
    "     \"mlflow_tracking_uri\": <mlflow_uri>,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state. Here's what you need to know:\n",
    "\n",
    "**Basic Status Overview**: The monitoring functionality in our system is designed to inform you whether your jobs are in a pending, running, done, or error state. This status update allows you to quickly assess the overall progress and detect any immediate issues that may require attention.\n",
    "\n",
    "Status interpretation:\n",
    "- \"Pending\": MONAI cloud is looking for resources and preparing the datasets. This can take quite a while, and depends on the size of the dataset.\n",
    "- \"Running\": MONAI cloud has submitted the job to the DGX. \n",
    "- \"Done\": The training is complete\n",
    "- \"Error\": There is some error in the job. User probably wants to download the job as a `.tar.gz` archive and inspect the detailed log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running jobs\n",
    "def wait_for_job(endpoint, headers, timeout):\n",
    "    start_time = time.time()\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    status = response.json()[\"status\"].title()\n",
    "    print(\"Waiting for job to complete...\")\n",
    "    print(status, end=\"\", flush=True)\n",
    "    while True:\n",
    "        if status not in [\"Pending\", \"Running\"]:\n",
    "            assert status == \"Done\", f\"Job failed with status: {status}\"\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if status_new != status:\n",
    "            status = status_new\n",
    "            print(f\"\\n{status}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"Job timeout after {timeout} seconds.\")\n",
    "            break\n",
    "    print(f\"\\nJob status: {status}\")\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{automl_experiment_id}/jobs/{automl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Generated Model Inference\n",
    "\n",
    "Users can easily deploy models trained via the Auto3DSeg pipeline for inference on their test datasets. This process involves selecting an AutoML-optimized model, tailored for high accuracy and efficiency in medical imaging tasks. The trained model is then applied to the test dataset, allowing users to evaluate its performance in real-world scenarios. This seamless integration from training to inference exemplifies the practical utility of Auto3DSeg in streamlining complex medical imaging analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. List the experiments and select the first generated Auto3DSeg experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "params = {\"user_only\": True, \"network_arch\": \"monai_segmentation\"}\n",
    "# you can use the assigned \"output_experiment_name\" in the previous steps to filter the experiments\n",
    "# params = {\"user_only\": True, \"network_arch\": \"monai_segmentation\", \"name\": \"Auto3DSegGenModel\"}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.status_code == 200, f\"List experiment failed, got {response.json()}.\"\n",
    "experiments = response.json()\n",
    "assert len(experiments) > 0, \"No experiments found!\"\n",
    "selected = \"x\"\n",
    "for m in experiments:\n",
    "    print(f'- {selected} {m[\"name\"]:25} : {m[\"id\"]} ({m[\"created_on\"]})')\n",
    "    selected = \" \"\n",
    "experiment_id = experiments[0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. [Optional] List more information about the selected experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get experiment info, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Run Inference**\n",
    "\n",
    "With the model and the `inference_dataset` prepared, users can prepare the payload data and submit an inference request as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"inference_dataset\": infer_dataset_id,\n",
    "        \"num_gpu\": 2,\n",
    "    },\n",
    "}\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create job failed, got {response.json()}.\"\n",
    "infer_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID:\", infer_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Check on the Inference Job**\n",
    "\n",
    "After the job is submitted, users can continue to use the APIs to check the status of the inference job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "wait_for_job(endpoint, headers, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Download the result of the Inference Job**\n",
    "\n",
    "Finally, when the jobs are completed, users can download the result to their local drive and examine the outputs, models, and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "# In order to download the job, the inference process should be finished\n",
    "if response.json()[\"status\"] == \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}:download\"\n",
    "\n",
    "    # Download the results\n",
    "    with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print(\"Downloading job results...\")\n",
    "        output_file = f\"{infer_job_id}.tar.gz\"\n",
    "        with open(output_file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Inference results are downloaded at {output_file}\")\n",
    "\n",
    "    assert os.path.exists(output_file), \"Download failed, archive has not been created.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment and datasets after jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel automl job and inference job if not Done. This step is required before cleaning data\n",
    "endpoint = f\"{base_url}/experiments/{automl_experiment_id}/jobs/{automl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "if response.json()[\"status\"] != \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{automl_experiment_id}/jobs/{automl_job_id}:cancel\"\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Cancel job {automl_job_id} failed, got {response.json()}.\"\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "if response.json()[\"status\"] != \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Cancel job {job_id} failed, got {response.json()}.\"\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{automl_experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete automl experiment failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete inference experiment failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete train dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{infer_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete inference dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, the combination of Auto3DSeg with the MONAI Cloud API and NVIDIA DGX Cloud marks a significant stride in medical imaging technology. It simplifies the 3D segmentation process and harnesses the potential of AutoML, making advanced medical imaging analysis more accessible and efficient for both beginners and experts. This integration, facilitating a smooth progression from model training to inference, showcases the practical and powerful capabilities of this approach in enhancing medical imaging workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
