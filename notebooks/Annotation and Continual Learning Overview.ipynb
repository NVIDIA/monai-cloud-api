{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06218d49-1d08-4e74-8224-5370d474d953",
   "metadata": {},
   "source": [
    "## Guide to Annotation and Continual Learning with NVIDIA MONAI Cloud APIs\n",
    "\n",
    "This guide delves into the processes of annotation and continual learning using NVIDIA MONAI Cloud APIs. As the bedrock of medical imaging, accurate annotations are pivotal, and the continual refinement of models ensures they deliver the best results over time. We'll walk through the various steps and considerations involved in this process.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Annotation%20and%20Continual%20Learning%20Overview.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Setup\n",
    "- Creating a New Dataset for Annotation\n",
    "- Configuring Annotation and Continual Learning Parameters\n",
    "- VISTA Workflows\n",
    "- Annotation Workflow\n",
    "- Stopping a Continual Learning Job\n",
    "- Stopping the Experiment from Realtime Inference mode\n",
    "- Check the Training Job Results\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Annotation and Continual Learning are core features of NVIDIA MONAI Cloud APIs, streamlining the process of refining datasets and enhancing model performance progressively. Continual learning leverages accumulated annotations to improve the model iteratively. This guide will assist you in setting up and optimizing these critical tasks.\n",
    "\n",
    "Before diving into annotation and continual learning, we're going to quickly create our dataset and experiment that will be used for the annotation workflow.  \n",
    "\n",
    "### What You Can Expect to Learn\n",
    "\n",
    "The objective is to demonstrate how you can utilize the APIs to ensure your models adapt and improve over time with new data inputs. We will show you how to configure your datasets, manage annotation tasks, and effectively employ continual learning strategies to maximize the accuracy and efficiency of your models. By the end of this notebook, you will have a solid understanding of the annotation process and continual learning mechanisms within the MONAI Cloud API platform, empowering you to initiate these practices in your own projects.\n",
    "\n",
    "\n",
    "**Note:** We're going to use the `realtime_infer` parameter when creating our experiment as that will automatically load the experiment and make sure it's ready for our annotation and continual learning workflow.\n",
    "\n",
    "We've covered these steps in-depth in our other notebooks, you can find them below. If you haven't already gone through those notebooks, we encourage you to go back and review those first.\n",
    "\n",
    "- [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb)\n",
    "- [Dataset Creation and Experiment Selection](./Dataset%20Creation%20and%20Experiment%20Selection.ipynb)\n",
    "- [Perform Real-time Inference](./Perform%20Real-time%20Inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd62867",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import requests\" || pip install -q \"requests\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71a940",
   "metadata": {},
   "source": [
    "#### Required Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3efbe6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "\n",
    "# Dicom Server\n",
    "dicom_web_endpoint = \"<DICOMWeb address>\" # Please fill it with the actual endpoint (usually ended with /dicom-web). For example \"http://127.0.0.1:8042/dicom-web\".\n",
    "dicom_client_id = \"<DICOMWeb user ID>\"    # If Authentication is enabled, then provide username, otherwise fill it with the default username \"orthanc\"\n",
    "dicom_client_secret = \"<DICOMWeb secret>\" # If Authentication is enabled, then provide password, otherwise fill it with the default password \"orthanc\"\n",
    "\n",
    "# The cloud storage type used in this notebook. Currently only support `aws` and `azure`.\n",
    "cloud_type = \"azure\" # cloud storage provider: aws or azure\n",
    "cloud_account = \"account_name\" # if cloud_type == \"aws\"  should be \"access_key\"\n",
    "cloud_secret = \"access_key\" # if cloud_type == \"aws\" should be \"secret_key\"\n",
    "\n",
    "# Cloud storage credentials. Needed for storing the data and results of the experiments.\n",
    "access_id = \"<user name for the remote storage object>\"  # Please fill it with the actual Access ID\n",
    "access_secret = \"<secret for the remote storage object>\"  # Please fill it with the actual Access Secret\n",
    "\n",
    "# Experiment Cloud Storage. This is the storage where your jobs and experiments data will be stored.\n",
    "cs_bucket = \"<bucket or container name to push experiment job data to>\"  # Please fill it with the actual bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf7db3",
   "metadata": {},
   "source": [
    "#### Login into NGC and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35baf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", json={\"ngc_api_key\": ngc_api_key})\n",
    "response.raise_for_status()\n",
    "assert \"user_id\" in response.json(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json(), \"token is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "ngc_org = \"iasixjqzw1hj\"  # This is the default org for MONAI users. Please select the correct org if you are not using the default one.\n",
    "base_url = f\"{api_url}/orgs/{ngc_org}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "print(\"API Calls will be forwarded to\", base_url)\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow = False  # If you want to use MLFlow, set this to True.\n",
    "mlflow_server_address = \"\"  # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\"  # For example \"my_experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f8b1",
   "metadata": {},
   "source": [
    "## Creating a New Dataset for Annotation\n",
    "\n",
    "We'll start by creating a new dataset for annotation. The dataset, hosted on a DICOMweb server, will be accessed using the `dicomweb` protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e59eec-e347-461b-866c-41c1ed9baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"mydataset\",\n",
    "    \"description\":\"a demo dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": f\"{dicom_web_endpoint}\",\n",
    "    \"client_id\": f\"{dicom_client_id}\",\n",
    "    \"client_secret\": f\"{dicom_client_secret}\",\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ad28c",
   "metadata": {},
   "source": [
    "## Creating a New Experiment for Annotation\n",
    "\n",
    "#### Find the base experiment for VISTA-3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List base experiments failed, got {response.text}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "vista3d_base_exps = [p for p in res[\"experiments\"] if p[\"network_arch\"] == \"monai_vista3d\"]\n",
    "assert len(vista3d_base_exps) > 0, \"No base experiment found for VISTA 3D bundle\"\n",
    "print(\"List of available base experiments for VISTA 3D bundle:\")\n",
    "for exp in vista3d_base_exps:\n",
    "    print(f\"  {exp['id']}: {exp['name']} v{exp['version']}\")\n",
    "base_experiment = sorted(vista3d_base_exps, key=lambda x: x[\"version\"])[-1]  # Take the latest version\n",
    "version = base_experiment[\"version\"]\n",
    "base_exp_vista = base_experiment[\"id\"]\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(f\"Base experiment ID for '{base_experiment['name']}' v{base_experiment['version']}: {base_exp_vista}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21845522",
   "metadata": {},
   "source": [
    "Next, we create a new experiment tailored for annotation, utilizing the `realtime_infer` parameter to ensure the readiness for inference and continual learning. We'll specify the `labels` to indicate what labels we want to continually learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cloud_details = {\n",
    "    \"cloud_type\": cloud_type,\n",
    "    \"cloud_file_type\": \"folder\",  # If the file is tar.gz key in \"file\", else \"folder\"\n",
    "    \"cloud_specific_details\": {\n",
    "        \"cloud_bucket_name\": cs_bucket,  # Bucket link to save files\n",
    "        cloud_account: access_id,  # Access and Secret for Azure\n",
    "        cloud_secret: access_secret,  # Access and Secret for Azure\n",
    "    }\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"my_vista\",\n",
    "    \"description\": \"based on vista\",\n",
    "    \"network_arch\": \"monai_vista3d\",\n",
    "    \"base_experiment\": [ base_exp_vista ],\n",
    "    \"inference_dataset\": dataset_id,\n",
    "    \"eval_dataset\": dataset_id,\n",
    "    \"train_datasets\": [ dataset_id ],\n",
    "    \"realtime_infer\": True, # Auto loads MONAI bundle and enables real-time inference\n",
    "    \"cloud_details\": experiment_cloud_details,\n",
    "    \"model_params\":{\n",
    "        \"labels\": {\n",
    "            \"1\": \"liver\",\n",
    "            \"2\": \"kidney\",\n",
    "            \"3\": \"spleen\",\n",
    "            \"4\": \"pancreas\",\n",
    "            \"5\": \"right kidney\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "print(\"Experiment creation succeeded with experiment ID:\", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f2995b-e4d7-4810-bd8a-0764326bcd7d",
   "metadata": {},
   "source": [
    "## Configuring Annotation and Continual Learning Parameters\n",
    "\n",
    "Continual learning is the backbone of keeping our models accurate and up-to-date. As new data is annotated, the model can learn and adapt. To initiate this process, we must define certain parameters to guide the system how and when to refine the model.\n",
    "\n",
    "With this job setup, the model will be fine-tuned with newly labeled samples after a specific number of notifications. A fine-tuned model can produce a better annotation results, thus enhancing the annotation efficiency.\n",
    "\n",
    "*Note: If you prefer to only annotate data without the continual learning process, you can simply skip this step. You can still use the annotation tools and workflows outlined in the upcoming sections independently.*\n",
    "\n",
    "### API Call for Continual Learning Job\n",
    "\n",
    "Below is the API call required to initiate a continual learning job for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1af77a-c109-48ea-bab1-784bc160542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"epochs\": 2,\n",
    "    \"val_interval\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"save_execute_config\": False\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "    \"action\": \"annotation\",\n",
    "    \"specs\": {\n",
    "        \"round_size\": 1,  # round_size: number of images to annotate in each round, e.g. notify at least 2 different image_ids\n",
    "        \"stop_criteria\": {\n",
    "            \"max_rounds\": 2,\n",
    "            \"key_metric\": 0.9,\n",
    "        },\n",
    "        \"train_spec\": train_spec,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run job failed, got {response.json()}.\"\n",
    "cl_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID: \", cl_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70528bc7-2af2-4965-b0da-5af6d97aa890",
   "metadata": {},
   "source": [
    "**Parameter Details**:\n",
    "- `round_size`: Specifies how many new annotations are needed to trigger a new fine-tuning round for the model.\n",
    "- `stop_criteria`: Criteria to decide when the continual learning job should cease. \n",
    "    - `max_rounds`: Determines the maximum rounds the job should run.\n",
    "    - `key_metric`: (Optional) If specified, the job will keep running until the designated evaluation metric reaches the value set.\n",
    "- `train_spec`: Overrides certain parameters in the model for this particular training. If you have an MLflow server set up, you can add its  parameters under tracking to enable logging metrics with MLflow.\n",
    "\n",
    "#### Check Job Status\n",
    "\n",
    "Ensure the continual learning job is up and running as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e64d92-d346-4071-9871-5b9b0f9287f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(endpoint, headers, timeout=1800, interval=5, target_status=\"Done\"):\n",
    "    \"\"\"Helper function to wait for job to reach target status.\"\"\"\n",
    "    expected = [\"Pending\", \"Running\", \"Done\"]\n",
    "    assert target_status in expected, f\"Invalid target status: {target_status}\"\n",
    "    status_before_target = expected[:expected.index(target_status)]\n",
    "    start_time = time.time()\n",
    "    print(f\"Waiting for job to reach state {target_status} ...\")\n",
    "    status = None\n",
    "    while True:\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"\\nJob timeout after {timeout} seconds with last status {status_new}.\")\n",
    "            break\n",
    "        elif status_new not in status_before_target:\n",
    "            assert status_new == target_status, f\"Job failed with status: {status_new}\"\n",
    "            print(f\"\\nJob reached target status: {status_new}\")\n",
    "            break\n",
    "        print(f\"\\n{status_new}\", end=\"\", flush=True) if status_new != status else print(\".\", end=\"\", flush=True)\n",
    "        status = status_new\n",
    "        time.sleep(interval)\n",
    "\n",
    " \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "wait_for_job(endpoint, headers, timeout=60, interval=1, target_status=\"Running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861fe31-54df-4d8e-95f4-5414d2a1f1d5",
   "metadata": {},
   "source": [
    "### Using MLflow to Monitor Metrics\n",
    "\n",
    "If you've set up MLflow and included the relevant parameters in your continual learning job, you can actively monitor the training metrics through the platform. This is invaluable for gauging the performance of your model in real-time and making timely interventions when necessary.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33996ee-34ed-4815-8075-b652242ab007",
   "metadata": {},
   "source": [
    "## VISTA Workflows\n",
    "\n",
    "Deep-dive into specific workflows that allow refined interaction with the model:\n",
    "\n",
    "1. **Segment All Classes**: Users can analyze an entire image without specific prompts, offering a comprehensive overview.\n",
    "2. **Using Class Prompts**: Users direct the model's focus towards one or more specific classes. Class-based segmentation can enable a specialized focus on a particular disease/organ.\n",
    "3. **Using Point Prompts**: Users specify a sequence of background and foreground clicks to guide the modelâ€™s focus, particularly when used together with class prompts.\n",
    "\n",
    "These workflows also integrate seamlessly with the OHIF Plugin for an enhanced visual experience, we'll walk through the OHIF experience below along with the accompanying API call used in the background.\n",
    "\n",
    "### Using Segment Everything\n",
    "By default, the VISTA-3D Experiment provides 132 classes and using the Auto Segmentation panel, you can run inferencing use all available classes.\n",
    "\n",
    "**Steps**\n",
    "1. Click the `run` button under the `Auto Segmentation panel` to obtain the segmentation mask for all classes.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/inference_as.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c39f3a-4b0e-4336-b8eb-52292c2a177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an inference image id with nextimage api\n",
    "data = {\n",
    "    \"action\": \"nextimage\"\n",
    "}\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Recommend image failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "inference_image_id = res[\"image\"]\n",
    "print(f\"Recommended Image to annotate: {inference_image_id}\")\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": {\n",
    "            \"label_prompt\": list(range(1, 133))  # inference all 132 classes\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b89e-0afe-4828-af51-50478649a188",
   "metadata": {},
   "source": [
    "### Using Class Prompts\n",
    "Instead of using all 132 labels, you can select a few labels that you're interested in and run inference only on those classes.  If you're using a customize version of VISTA-3D as referenced in our [Dataset Creation and Experiment Selection](./Dataset-Creation-and-Experiment-Selection.ipynb) notebook, you'll see only the classes you created with the model listed in this section.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Class Prompts` panel.\n",
    " 2. Select classes that you want to inference with class prompts.\n",
    " 3. Click the `Run` button to get the inference result.\n",
    "\n",
    "![Inference Point Prompts](./end2end_pic/inference_class_prompts.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    "![Inference Point Prompts Result](./end2end_pic/inference_class_prompts_res.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f37499-964f-40be-b08a-9f19858084de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"label_prompt\": [1, 2, 3, 4, 5], # Whichever classes were selected\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": bundle_params,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b3c06-253d-4ad2-bb4c-bde506bdbf15",
   "metadata": {},
   "source": [
    "### Using Point Prompts\n",
    "Last, instead of using only class prompts, you can use point+class prompts.  This allows you to add points to the indicated classes to help guide the model and refine your segmentation using an interactive workflow.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Point Prompts` panel.\n",
    " 2. Select a class that you want to inference with point prompts.\n",
    " 3. Add some point to the image where you want to get the mask by clicking.\n",
    " 4. Click the `Run` button to get the inference result.\n",
    "\n",
    " ![Inference Point Prompts](./end2end_pic/inference_point.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    " ![Inference Point Prompts Result](./end2end_pic/inference_point_res.png)\n",
    "\n",
    "If you want to clear some points, you can either clear specific class points or clear all points by clicking the `Clear Points` or `Clear All Points` button.\n",
    "\n",
    "![Clear Points](./end2end_pic/clearpoints.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc775db-252a-4fea-adcd-12d073c35403",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"points\": [[20,20,20], [20, 40, 60]],\n",
    "    \"point_labels\": [2, 2],\n",
    "    \"label_prompt\": [3],\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": bundle_params,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1881271-8226-489a-be1b-951c8a1defc5",
   "metadata": {},
   "source": [
    "## Annotation Workflow\n",
    "\n",
    "Annotating medical images efficiently and precisely is a multi-step process. Here's a breakdown of the typical workflow you'd employ when using NVIDIA MONAI Cloud APIs and OHIF. We'll cover any relevant APIs not already covered as we walk through the workflow.\n",
    "\n",
    "`Load Image` --> `Run Inference` --> `Annotate/Fix Annotation` --> `Save /Notify` --> `Repeat`\n",
    "\n",
    "### 1. **Load Image**\n",
    "\n",
    "Begin by loading the desired medical image that you wish to annotate. If you're using OHIF, you'll see the study list and can select a patient the annotate.  Make sure to use the `MONAI Service` to load the NVIDIA MONAI Cloud API plugin.\n",
    "\n",
    "![Select an image](end2end_pic/selectanimage.png)\n",
    "\n",
    "If you're using the API directly, you can use the `nextimage` endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a136bb2-fbe0-4f31-8bed-475dddaac959",
   "metadata": {},
   "source": [
    "### 2. **Run Inferencing Using Selected Method**\n",
    "\n",
    "Choose one of the inferencing methods discussed above:\n",
    "\n",
    "1. **Segment All Classes**\n",
    "2. **Using Class Prompts**\n",
    "3. **Using Point Prompts**\n",
    "\n",
    "Once you've picked your preferred method, run the inference to get an initial annotation.\n",
    "\n",
    "![allclass](./end2end_pic/allclassohif.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d341faa-6025-48fb-b6e9-5f1cb85d4428",
   "metadata": {},
   "source": [
    "### 3. **Annotate / Refine Annotations**\n",
    "\n",
    "With the initial mask in place, you might notice areas that require manual tweaking. Use the provided annotation tools to:\n",
    "\n",
    "- Refine boundaries\n",
    "- Add or remove regions\n",
    "\n",
    "This step ensures that your annotations are as accurate as possible.\n",
    "\n",
    "**Steps**\n",
    "1. Click the Segmentation button.\n",
    "2. Select a class of segmentation that needs to be updated.\n",
    "3. Select a segmentation tool.\n",
    "4. Update the segmentation with this tool.\n",
    "\n",
    "![Annotate](./end2end_pic/annotate.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46d4ac51-3f01-4876-a1c6-f9c15c5baefd",
   "metadata": {},
   "source": [
    "### 4. **Save and Notify the Server**\n",
    "\n",
    "Once you're satisfied with your annotations, the first step is to save the annotated image, ensuring that your work is captured. This will write back the image using the DICOMWeb protocal back to your datastore.\n",
    "\n",
    "![Save Label](./end2end_pic/savelabel.png)\n",
    "\n",
    "Next, notify the server that an image has been annotated. This step is crucial for continual learning. The system will take note of the new annotations and after the indicated number of annotated images it will use them to improve the model over time.\n",
    "\n",
    "![Notify](end2end_pic/notify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560106a4",
   "metadata": {},
   "source": [
    "The associated API call run when you click the `Notify Server` button is below:\n",
    "\n",
    "```python\n",
    "# After uploading a DICOM Seg into DICOM Web\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "label_id = \"<series_id_1>\"\n",
    "data = {\n",
    "    \"action\": \"notify\",\n",
    "    \"specs\": {\n",
    "        \"added\": {\n",
    "            \"image\": inference_image_id,\n",
    "            \"label\": label_id,\n",
    "        },\n",
    "        \"updated\": [],\n",
    "        \"removed\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Notified.\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218b343-5897-4b3d-8ddb-f0e21748f1b6",
   "metadata": {},
   "source": [
    "### 5. **Repeat**\n",
    "\n",
    "Continue the process for all the images in your dataset. With each iteration, not only do you expand your annotated dataset, but you also contribute to the model's learning, making future annotations even more accurate.\n",
    "\n",
    "You can check the job log of continual learning by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "status = response.json()[\"status\"].title()\n",
    "if status in [\"Running\", \"Done\", \"Error\"]:\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}/logs\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job logs, got {response.text}.\"\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Job status: {status}, logs are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c45c9-cb76-4890-ac40-7c4d8c3697fc",
   "metadata": {},
   "source": [
    "## Stopping a Continual Learning Job\n",
    "\n",
    "As your model refines itself over time using continual learning, there might come a point where you need to halt the ongoing CL job. Whether you're satisfied with the model's performance or have other reasons, here's how you can stop the CL job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b4d8-4353-408b-9c6a-99d159facffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually stop the CL job. No need to execute this cell if the job has reached the stop criteria.\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "if response.json()[\"status\"] != \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}:cancel\"\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"cancel job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbe2fa",
   "metadata": {},
   "source": [
    "## Stopping the Experiment from Realtime Inference mode\n",
    "\n",
    "When the experiment is created with `realtime_infer` as `True`, it will reserve one GPU to process the inference requests.\n",
    "\n",
    "After we have finished the inference process, we would like to release the GPU resource for other tasks.\n",
    "\n",
    "To achieve this, we can switch the `realtime_infer` from `True` to `False`.\n",
    "\n",
    "Note: this step is irreversible, which means you can't set the `realtime_infer` from `False` to `True`. To bootstrap another inference, you will have to create another experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa275dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"realtime_infer\": False,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.patch(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 200, f\"stop job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b45c32-9d44-4bd2-896e-884425e0447c",
   "metadata": {},
   "source": [
    "## Check the Training Job Results\n",
    "\n",
    "After you've trained the model, you might want to check the details.  Here's how you can accomplish that using the following APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all jobs and pick one job that meets your requirement.\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"List all jobs failed, got {response.json()}.\"\n",
    "job_metas = response.json()[\"jobs\"]\n",
    "for job_meta in job_metas:\n",
    "    if job_meta[\"id\"] == cl_job_id:\n",
    "        print(\"Continual Learning Job status: \", job_meta[\"status\"])\n",
    "    else:\n",
    "        train_job_id = job_meta[\"id\"]\n",
    "        print(f\"Training Job {train_job_id} status: \", job_meta[\"status\"])\n",
    "        if job_meta[\"status\"] == \"Done\":\n",
    "            print(f\"Training Job {train_job_id} completed with key metric: \", job_meta[\"result\"][\"key_metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5822ee2-42f5-490a-aeca-a975504392a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a job id from the last cell output. For example, choose train_job_id\n",
    "if \"train_job_id\" in locals():\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.json()[\"status\"] != \"Done\":\n",
    "        endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}:cancel\"\n",
    "        response = requests.post(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"cancel train job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d5081",
   "metadata": {},
   "source": [
    "### Detailed Logging Through Download API\n",
    "\n",
    "For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6be258",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"download_job_id\" in locals():\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{download_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.json()[\"status\"] in [\"Running\", \"Done\", \"Error\"]:\n",
    "        # Download the job log\n",
    "        endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{download_job_id}/logs\"\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Failed to download job log, got {response.json()}.\"\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430318c",
   "metadata": {},
   "source": [
    "### Check the job results (checkpoint, scripts, logs, etc.)\n",
    "\n",
    "You'll find the results in the cloud storage bucket you specified when creating the experiment. The results will include the model checkpoints, scripts, logs, and other relevant data.\n",
    "\n",
    "The path to the results will be in the following format:\n",
    "\n",
    "```python\n",
    "f\"{bucket_name}/shared/orgs/{ngc_org}/users/{user_id}/jobs/{job_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729f8e0",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07aef753",
   "metadata": {},
   "source": [
    "After completing your jobs, it's good practice to clean up any experiments and datasets that are no longer needed. This helps maintain an organized workspace and ensures efficient resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf56d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59312363-d36e-440f-9044-185882c19517",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Remember, NVIDIA MONAI Cloud APIs are designed to make the process intuitive and efficient, allowing you to concentrate on the quality of your annotations while the technical details are managed in the background. Take full advantage of continual learning and annotation with NVIDIA MONAI Cloud APIs to achieve excellence in medical imaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "az-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
