{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06218d49-1d08-4e74-8224-5370d474d953",
   "metadata": {},
   "source": [
    "## Guide to Annotation and Continuous Learning with NVIDIA MONAI Cloud APIs\n",
    "\n",
    "In this guide, we delve deep into the process of annotation and continuous learning using NVIDIA MONAI Cloud APIs. As the bedrock of medical imaging, accurate annotations are pivotal, and the continuous refinement of models ensures they deliver the best results over time. We'll walk through the various steps and considerations involved in this process.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Dataset and Experiment Setup\n",
    "- Configuring Annotation and Continuous Learning Parameters\n",
    "- VISTA Workflows\n",
    "- Annotation Workflow\n",
    "- Stopping a Continuous Learning Job\n",
    "- Exporting the Experiment\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Annotation and Continuous Learning are core features of NVIDIA MONAI Cloud APIs, streamlining the process of refining datasets and enhancing model performance over time. Continuous learning leverages accumulated annotations to improve the model iteratively. This guide will walk you through the process of setting up and optimizing these tasks.\n",
    "\n",
    "## Dataset and Experiment Setup\n",
    "\n",
    "Before diving into annotation and continuous learning, we're going to quickly create our dataset and experiment that will be used for the annotation workflow.  \n",
    "\n",
    "**Note:** We're going to use the `realtime_infer` parameter when creating our experiment as that will automatically load the experiment and make sure it's ready for our annotation and continuous learning workflow.\n",
    "\n",
    "We've covered these steps in-depth in our other notebooks, you can find them below. If you haven't already gone through those notebooks, we encourge you to go back and review those first.\n",
    "\n",
    "- [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb)\n",
    "- [Dataset Creation and Experiment Selection](./Dataset%20Creation%20and%20Experiment%20Selection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3efbe6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"<MONAI Cloud API URL>\"\n",
    "ngc_api_key = \"<NGC API Key>\"\n",
    "# Dicom Server\n",
    "dicom_web_endpoint = \"<DICOMWeb address>\" # For example \"http://127.0.0.1:8042/dicom-web\".\n",
    "dicom_client_id = \"<DICOMWeb user ID>\"    # If Authentication is enabled, then provide username\n",
    "dicom_client_secret = \"<DICOMWeb secret>\" # If Authentication is enabled, then provide password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e59eec-e347-461b-866c-41c1ed9baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGC UID \n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "print(response.status_code)\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow = False\n",
    "mlflow_server_address = \"\" # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\" # For example \"my_experiment\"\n",
    "\n",
    "data = {\n",
    "    \"name\": \"mydataset\",\n",
    "    \"description\":\"a demo dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": f\"{dicom_web_endpoint}\",\n",
    "    \"client_id\": f\"{dicom_client_id}\",\n",
    "    \"client_secret\": f\"{dicom_client_secret}\",\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n",
    "    \n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List Base Experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "ptm_vista = [p for p in res if p[\"network_arch\"] == \"monai_vista3d\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for VISTA Experiment: {ptm_vista}\")\n",
    "    \n",
    "data = {\n",
    "  \"name\": \"my_vista\",\n",
    "  \"description\": \"based on vista\",\n",
    "  \"network_arch\": \"monai_vista3d\",\n",
    "  \"base_experiment\": [ ptm_vista ],\n",
    "  \"inference_dataset\": dataset_id,\n",
    "  \"eval_dataset\": dataset_id,\n",
    "  \"train_datasets\": [ dataset_id ],\n",
    "  \"realtime_infer\": True, # Auto loads MONAI bundle and enables real-time inference\n",
    "  \"model_params\":{\n",
    "      \"labels\": {\n",
    "            \"1\": \"liver\",\n",
    "            \"2\": \"kidney\",\n",
    "            \"3\": \"spleen\",\n",
    "            \"4\": \"pancreas\",\n",
    "            \"5\": \"right kidney\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "model_network = res[\"network_arch\"]\n",
    "print(\"Experiment creation succeeded with experiment ID:\", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f2995b-e4d7-4810-bd8a-0764326bcd7d",
   "metadata": {},
   "source": [
    "## Configuring Annotation and Continuous Learning Parameters\n",
    "\n",
    "Continuous learning is the backbone of keeping our models accurate and up-to-date. As new data is annotated, the model has the potential to learn and adapt. However, to kick off this process, we need to specify certain parameters that inform the system how and when to refine the model.\n",
    "\n",
    "With this job, the model will be fine tuned with new labeled samples after several notifications. A fine tuned model can generate a better annotation results, therefore improving the annotation efficiency.\n",
    "\n",
    "*If you prefer to only annotate data without the continuous learning process, you can simply skip this step. You can still use the annotation tools and workflows outlined in the upcoming sections independently.*\n",
    "\n",
    "### API Call for Continuous Learning Job\n",
    "\n",
    "Below, we provide the API call needed to create a continuous learning job for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1af77a-c109-48ea-bab1-784bc160542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"epochs\": 10,\n",
    "    \"val_interval\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"train#handlers#-1#artifacts\": None\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "    \"action\": \"annotation\",\n",
    "    \"specs\": {\n",
    "        \"round_size\": 2,  # round_size: number of images to annotate in each round, e.g. notify at least 2 different image_ids\n",
    "        \"stop_criteria\": {\n",
    "            \"max_rounds\": 2,\n",
    "            \"key_metric\": 0.9,\n",
    "        },\n",
    "        \"train_spec\": train_spec,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run job failed, got {response.json()}.\"\n",
    "cl_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID: \", cl_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70528bc7-2af2-4965-b0da-5af6d97aa890",
   "metadata": {},
   "source": [
    "**Parameter Details**:\n",
    "- `round_size`: Specifies how many new annotations are needed to trigger a new fine-tuning round for the model.\n",
    "- `stop_criteria`: Criteria to decide when the continuous learning job should cease. \n",
    "    - `max_rounds`: Determines the maximum rounds the job should run.\n",
    "    - `key_metric`: (Optional) If specified, the job will keep running until the designated evaluation metric reaches the value set.\n",
    "- `train_spec`: Overrides certain parameters in the model for this particular training. If you have an MLflow server set up, you can add its  parameters under tracking to enable logging metrics with MLflow.\n",
    "\n",
    "#### Check Job Status\n",
    "\n",
    "Ensure the continuous learning job is up and running as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e64d92-d346-4071-9871-5b9b0f9287f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"Pending\"\n",
    "while status != \"Running\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    status = response.json()[\"status\"]\n",
    "    print(\"Continuous Learning/Annotation Job status: \", status)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861fe31-54df-4d8e-95f4-5414d2a1f1d5",
   "metadata": {},
   "source": [
    "### Using MLflow to Monitor Metrics\n",
    "\n",
    "If you've set up MLflow and included the relevant parameters in your continuous learning job, you can actively monitor the training metrics through the platform. This is invaluable for gauging the performance of your model in real-time and making timely interventions when necessary.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33996ee-34ed-4815-8075-b652242ab007",
   "metadata": {},
   "source": [
    "## VISTA Workflows\n",
    "\n",
    "Deep-dive into specific workflows that allow refined interaction with the model:\n",
    "\n",
    "1. **Segment All Classes**: Users can analyze an entire image without specific prompts, offering a comprehensive overview.\n",
    "2. **Using Class Prompts**: Users direct the model's focus towards one or more specific classes. Class-based segmentation can enable a specialized focus on a particular disease/organ.\n",
    "3. **Using Point Prompts**: Users specify a sequence of background and foreground clicks to guide the model’s focus, particularly when used together with class prompts.\n",
    "\n",
    "These workflows also integrate seamlessly with the OHIF Plugin for an enhanced visual experience, we'll walk through the OHIF experience below along with the accompanying API call used in the background.\n",
    "\n",
    "### Using Segment Everything\n",
    "By default, the VISTA-3D Experiment provides 118 classes and using the Auto Segmentation panel, you can run inferencing use all available classes.\n",
    "\n",
    "**Steps**\n",
    "1. Click the `run` button under the `Auto Segmentation panel` to obtain the segmentation mask for all classes.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/inference_as.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c39f3a-4b0e-4336-b8eb-52292c2a177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an inference image id with nextimage api\n",
    "data = {\n",
    "    \"action\": \"nextimage\"\n",
    "}\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Recommend image failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "inference_image_id = res[\"image\"]\n",
    "print(f\"Recommended Image to annotate: {inference_image_id}\")\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": {\n",
    "            \"label_prompt\": list(range(1, 118))  # inference all 117 classes\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b89e-0afe-4828-af51-50478649a188",
   "metadata": {},
   "source": [
    "### Using Class Prompts\n",
    "Instead of using all 118 labels, you can select a few labels that you're interested in and run inference only on those classes.  If you're using a customize version of VISTA-3D as referenced in our [Dataset Creation and Experiment Selection](./Dataset-Creation-and-Experiment-Selection.ipynb) notebook, you'll see only the classes you created with the model listed in this section.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Class Prompts` panel.\n",
    " 2. Select classes that you want to inference with class prompts.\n",
    " 3. Click the `Run` button to get the inference result.\n",
    "\n",
    "![Inference Point Prompts](./end2end_pic/inference_class_prompts.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    "![Inference Point Prompts Result](./end2end_pic/inference_class_prompts_res.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f37499-964f-40be-b08a-9f19858084de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"label_prompt\": [1, 2, 3, 4, 5], # Whichever classes were selected\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": bundle_params,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b3c06-253d-4ad2-bb4c-bde506bdbf15",
   "metadata": {},
   "source": [
    "### Using Point Prompts\n",
    "Last, instead of using only class prompts, you can use point+class prompts.  This allows you to add points to the indicated classes to help guide the model and refine your segmentation using an interactive workflow.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Point Prompts` panel.\n",
    " 2. Select a class that you want to inference with point prompts.\n",
    " 3. Add some point to the image where you want to get the mask by clicking.\n",
    " 4. Click the `Run` button to get the inference result.\n",
    "\n",
    " ![Inference Point Prompts](./end2end_pic/inference_point.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    " ![Inference Point Prompts Result](./end2end_pic/inference_point_res.png)\n",
    "\n",
    "If you want to clear some points, you can either clear specific class points or clear all points by clicking the `Clear Points` or `Clear All Points` button.\n",
    "\n",
    "![Clear Points](./end2end_pic/clearpoints.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc775db-252a-4fea-adcd-12d073c35403",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"points\": [[20,20,20], [20, 40, 60]],\n",
    "    \"point_labels\": [2, 2],\n",
    "    \"label_prompt\": [2],\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": bundle_params,\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1881271-8226-489a-be1b-951c8a1defc5",
   "metadata": {},
   "source": [
    "## Annotation Workflow\n",
    "\n",
    "Annotating medical images efficiently and precisely is a multi-step process. Here's a breakdown of the typical workflow you'd employ when using NVIDIA MONAI Cloud APIs and OHIF. We'll cover any relevant APIs not already covered as we walk through the workflow.\n",
    "\n",
    "`Load Image` --> `Run Inference` --> `Annotate/Fix Annotation` --> `Save /Notify` --> `Repeat`\n",
    "\n",
    "### 1. **Load Image**\n",
    "\n",
    "Begin by loading the desired medical image that you wish to annotate. If you're using OHIF, you'll see the study list and can select a patient the annotate.  Make sure to use the `MONAI Service` to load the NVIDIA MONAI Cloud API plugin.\n",
    "\n",
    "![Select an image](end2end_pic/selectanimage.png)\n",
    "\n",
    "If you're using the API directly, you can use the `nextimage` endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a136bb2-fbe0-4f31-8bed-475dddaac959",
   "metadata": {},
   "source": [
    "### 2. **Run Inferencing Using Selected Method**\n",
    "\n",
    "Choose one of the inferencing methods discussed above:\n",
    "\n",
    "1. **Segment All Classes**\n",
    "2. **Using Class Prompts**\n",
    "3. **Using Point Prompts**\n",
    "\n",
    "Once you've picked your preferred method, run the inference to get an initial annotation.\n",
    "\n",
    "![allclass](./end2end_pic/allclassohif.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d341faa-6025-48fb-b6e9-5f1cb85d4428",
   "metadata": {},
   "source": [
    "### 3. **Annotate / Refine Annotations**\n",
    "\n",
    "With the initial mask in place, you might notice areas that require manual tweaking. Use the provided annotation tools to:\n",
    "\n",
    "- Refine boundaries\n",
    "- Add or remove regions\n",
    "\n",
    "This step ensures that your annotations are as accurate as possible.\n",
    "\n",
    "**Steps**\n",
    "1. Click the Segmentation button.\n",
    "2. Select a class of segmentation that needs to be updated.\n",
    "3. Select a segmentation tool.\n",
    "4. Update the segmentation with this tool.\n",
    "\n",
    "![Annotate](./end2end_pic/annotate.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46d4ac51-3f01-4876-a1c6-f9c15c5baefd",
   "metadata": {},
   "source": [
    "### 4. **Save and Notify the Server**\n",
    "\n",
    "Once you're satisfied with your annotations, the first step is to save the annotated image, ensuring that your work is captured. This will write back the image using the DICOMWeb protocal back to your datastore.\n",
    "\n",
    "![Save Label](./end2end_pic/savelabel.png)\n",
    "\n",
    "Next, notify the server that an image has been annotated. This step is crucial for continuous learning. The system will take note of the new annotations and after the indicated number of annotated images it will use them to improve the model over time.\n",
    "\n",
    "![Notify](end2end_pic/notify.png)\n",
    "\n",
    "The associated API call run when you click the `Notify Server` button is below:\n",
    "\n",
    "```python\n",
    "# After uploading a DICOM Seg into DICOM Web\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "label_id = \"<series_id_1>\"\n",
    "data = {\n",
    "    \"action\": \"notify\"\n",
    "    \"specs\": {\n",
    "        \"added\": {\n",
    "            \"image\": inference_image_id,\n",
    "            \"label\": label_id,\n",
    "        },\n",
    "        \"updated\": [],\n",
    "        \"removed\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Notified.\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218b343-5897-4b3d-8ddb-f0e21748f1b6",
   "metadata": {},
   "source": [
    "### 5. **Repeat**\n",
    "\n",
    "Continue the process for all the images in your dataset. With each iteration, not only do you expand your annotated dataset, but you also contribute to the model's learning, making future annotations even more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c45c9-cb76-4890-ac40-7c4d8c3697fc",
   "metadata": {},
   "source": [
    "## Stopping a Continuous Learning Job\n",
    "\n",
    "As your model refines itself over time using continuous learning, there might come a point where you need to halt the ongoing CL job. Whether you're satisfied with the model's performance or have other reasons, here's how you can stop the CL job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b4d8-4353-408b-9c6a-99d159facffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually stop the CL job. No need to execute this cell if the job has reached the stop criteria.\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{cl_job_id}:cancel\"\n",
    "response = requests.post(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"cancel job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbe2fa",
   "metadata": {},
   "source": [
    "## Stopping the experiment from Realtime Inference mode\n",
    "\n",
    "When the experiment is created with `realtime_infer` as `True`, it will reserve one GPU to process the inference requests.\n",
    "\n",
    "After we have finished the inference process, we would like to release the GPU resource for other tasks.\n",
    "\n",
    "To achieve this, we can switch the `realtime_infer` from `True` to `False`.\n",
    "\n",
    "Note: this step is irreversible, which means you can't set the `realtime_infer` from `False` to `True`. To bootstrap another inference, you will have to create another experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa275dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"realtime_infer\": False,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.patch(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 200, f\"stop job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b45c32-9d44-4bd2-896e-884425e0447c",
   "metadata": {},
   "source": [
    "## <a id=\"Exporting-the-Experiment\">Exporting the Experiment</a>\n",
    "\n",
    "After you've trained the model, you might want to export it for various purposes.  Here's how you can accomplish that using the following APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all jobs and pick one job that meets your requirement.\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"List all jobs failed, got {response.json()}.\"\n",
    "job_metas = response.json()\n",
    "for job_meta in job_metas:\n",
    "    if job_meta[\"id\"] == cl_job_id:\n",
    "        print(\"Continuous Learning Job status: \", job_meta[\"status\"])\n",
    "    else:\n",
    "        train_id = job_meta[\"id\"]\n",
    "        print(f\"Training Job {train_id} status: \", job_meta[\"status\"])\n",
    "        if job_meta[\"status\"] == \"Done\":\n",
    "            print(f\"Training Job {train_id} with ID {job_meta['id']} metric: \", job_meta[\"result\"][\"key_metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5822ee2-42f5-490a-aeca-a975504392a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a job id from the last cell output. For example, choose train_id\n",
    "if \"train_id\" in locals():\n",
    "    download_job_id = train_id\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{download_job_id}:download\"\n",
    "    response = requests.get(endpoint, data=json.dumps({\"export_type\": \"monai_bundle\"}), headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to export job, got {response.json()}.\"\n",
    "    with open(f\"{download_job_id}.tar.gz\", \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d5081",
   "metadata": {},
   "source": [
    "### Detailed Logging Through Download API\n",
    "\n",
    "For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs, model checkpoints, and data outputs, which are instrumental for troubleshooting, in-depth analysis, and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6be258",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"download_job_id\" in locals():\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{download_job_id}:download\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    assert response.status_code == 200, f\"Failed to download job, got {response.json()}.\"\n",
    "    #save to file\n",
    "    attachment_data = response.content\n",
    "    with open(f\"{download_job_id}.tar.gz\", 'wb') as f:\n",
    "        f.write(attachment_data)\n",
    "    print(f\"Bundle training results are downloaded as {download_job_id}.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59312363-d36e-440f-9044-185882c19517",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Remember, the NVIDIA MONAI Cloud APIs are designed to streamline this process, making it intuitive and efficient. As you work through these steps, the platform aids you, ensuring that you can focus on the quality of annotations while the technical details are handled seamlessly in the background.\n",
    "\n",
    "Make the most of continuous learning and annotation with the NVIDIA MONAI Cloud APIs. This iterative refinement paves the way to excellence in medical imaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573a66c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "az-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
