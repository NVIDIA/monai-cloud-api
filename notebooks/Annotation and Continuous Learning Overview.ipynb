{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06218d49-1d08-4e74-8224-5370d474d953",
   "metadata": {},
   "source": [
    "## Guide to Annotation and Continuous Learning with NVIDIA MONAI Cloud APIs\n",
    "\n",
    "In this guide, we delve deep into the process of annotation and continuous learning using NVIDIA MONAI Cloud APIs. As the bedrock of medical imaging, accurate annotations are pivotal, and the continuous refinement of models ensures they deliver the best results over time. We'll walk through the various steps and considerations involved in this process.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Dataset and Model Setup](#Dataset-and-Model-Setup)\n",
    "- [Configuring Annotation and Continuous Learning Parameters](#Configuring-Annotation-and-Continuous-Learning-Parameters)\n",
    "- [VISTA Workflows](#VISTA-Workflows)\n",
    "- [Annotation Workflow](#Annotation-Workflow)\n",
    "- [Stopping a Continuous Learning Job](#Stopping-a-Continuous-Learning-Job)\n",
    "- [Exporting the Model](#Exporting-the-Model)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Annotation and Continuous Learning are core features of NVIDIA MONAI Cloud APIs, streamlining the process of refining datasets and enhancing model performance over time. Continuous learning leverages accumulated annotations to improve the model iteratively. This guide will walk you through the process of setting up and optimizing these tasks.\n",
    "\n",
    "## Dataset and Model Setup\n",
    "\n",
    "Before diving into annotation and continuous learning, we're going to quickly create our dataset and model that will be used for the annotation workflow.  \n",
    "\n",
    "**Note:** We're going to use the `realtime_infer` parameter when creating our model as that will automatically load the model and make sure it's ready for our annotation and continuous learning workflow.\n",
    "\n",
    "We've covered these steps in-depth in our other notebooks, you can find them below. If you haven't already gone through those notebooks, we encourge you to go back and review those first.\n",
    "\n",
    "- [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb)\n",
    "- [Dataset Creation and Model Selection](./Dataset%20Creation%20and%20Model%20Selection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e59eec-e347-461b-866c-41c1ed9baa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation succeeded with dataset ID:  62ed9367-56df-4813-b85a-b78b5e897096\n",
      "---------------------------------\n",
      "\n",
      "{\n",
      "  \"actions\": [\n",
      "    \"nextimage\",\n",
      "    \"cacheimage\",\n",
      "    \"notify\"\n",
      "  ],\n",
      "  \"created_on\": \"2023-10-26T02:51:55.932848\",\n",
      "  \"description\": \"a demo dataset\",\n",
      "  \"format\": \"monai\",\n",
      "  \"id\": \"62ed9367-56df-4813-b85a-b78b5e897096\",\n",
      "  \"jobs\": [],\n",
      "  \"last_modified\": \"2023-10-26T02:51:55.932856\",\n",
      "  \"location\": \"http://20.65.216.168:8042/dicom-web\",\n",
      "  \"logo\": \"https://www.nvidia.com\",\n",
      "  \"name\": \"mydataset\",\n",
      "  \"type\": \"semantic_segmentation\",\n",
      "  \"version\": \"1.0.0\"\n",
      "}\n",
      "PTM ID for VISTA Model: 543a531e-4533-4444-9664-f315a1e20645\n",
      "Model creation succeeded with model ID: 669a074f-17b5-417e-89c1-a4e367292c2d\n",
      "---------------------------------\n",
      "\n",
      "{\n",
      "  \"actions\": [\n",
      "    \"train\",\n",
      "    \"inference\",\n",
      "    \"annotation\"\n",
      "  ],\n",
      "  \"additional_id_info\": null,\n",
      "  \"automl_add_hyperparameters\": \"\",\n",
      "  \"automl_algorithm\": null,\n",
      "  \"automl_enabled\": false,\n",
      "  \"automl_remove_hyperparameters\": \"\",\n",
      "  \"calibration_dataset\": null,\n",
      "  \"created_on\": \"2023-10-26T02:51:56.257401\",\n",
      "  \"dataset_type\": \"semantic_segmentation\",\n",
      "  \"description\": \"based on vista\",\n",
      "  \"encryption_key\": \"tlt_encode\",\n",
      "  \"eval_dataset\": \"62ed9367-56df-4813-b85a-b78b5e897096\",\n",
      "  \"id\": \"669a074f-17b5-417e-89c1-a4e367292c2d\",\n",
      "  \"inference_dataset\": \"62ed9367-56df-4813-b85a-b78b5e897096\",\n",
      "  \"jobs\": [],\n",
      "  \"last_modified\": \"2023-10-26T02:51:56.257406\",\n",
      "  \"logo\": \"https://www.nvidia.com\",\n",
      "  \"metric\": null,\n",
      "  \"model_params\": {\n",
      "    \"labels\": {\n",
      "      \"1\": \"liver\",\n",
      "      \"10\": \"gallbladder\",\n",
      "      \"100\": \"right clavicula\",\n",
      "      \"101\": \"left femur\",\n",
      "      \"102\": \"right femur\",\n",
      "      \"103\": \"left hip\",\n",
      "      \"104\": \"right hip\",\n",
      "      \"105\": \"sacrum\",\n",
      "      \"106\": \"face\",\n",
      "      \"107\": \"left gluteus maximus\",\n",
      "      \"108\": \"right gluteus maximus\",\n",
      "      \"109\": \"left gluteus medius\",\n",
      "      \"11\": \"esophagus\",\n",
      "      \"110\": \"right gluteus medius\",\n",
      "      \"111\": \"left gluteus minimus\",\n",
      "      \"112\": \"right gluteus minimus\",\n",
      "      \"113\": \"left autochthon\",\n",
      "      \"114\": \"right autochthon\",\n",
      "      \"115\": \"left iliopsoas\",\n",
      "      \"116\": \"right iliopsoas\",\n",
      "      \"117\": \"bone lesion\",\n",
      "      \"12\": \"stomach\",\n",
      "      \"13\": \"duodenum\",\n",
      "      \"14\": \"left kidney\",\n",
      "      \"15\": \"postcava\",\n",
      "      \"16\": \"bladder\",\n",
      "      \"17\": \"prostate or uterus\",\n",
      "      \"18\": \"portal vein and splenic vein\",\n",
      "      \"19\": \"uterus\",\n",
      "      \"2\": \"kidney\",\n",
      "      \"20\": \"rectum\",\n",
      "      \"21\": \"small bowel\",\n",
      "      \"22\": \"lung\",\n",
      "      \"23\": \"bone\",\n",
      "      \"24\": \"brain\",\n",
      "      \"25\": \"lung tumor\",\n",
      "      \"26\": \"pancreatic tumor\",\n",
      "      \"27\": \"hepatic vessel\",\n",
      "      \"28\": \"hepatic tumor\",\n",
      "      \"29\": \"colon cancer primaries\",\n",
      "      \"3\": \"spleen\",\n",
      "      \"30\": \"left lung upper lobe\",\n",
      "      \"31\": \"left lung lower lobe\",\n",
      "      \"32\": \"right lung upper lobe\",\n",
      "      \"33\": \"right lung middle lobe\",\n",
      "      \"34\": \"right lung lower lobe\",\n",
      "      \"35\": \"vertebrae l5\",\n",
      "      \"36\": \"vertebrae l4\",\n",
      "      \"37\": \"vertebrae l3\",\n",
      "      \"38\": \"vertebrae l2\",\n",
      "      \"39\": \"vertebrae l1\",\n",
      "      \"4\": \"pancreas\",\n",
      "      \"40\": \"vertebrae t12\",\n",
      "      \"41\": \"vertebrae t11\",\n",
      "      \"42\": \"vertebrae t10\",\n",
      "      \"43\": \"vertebrae t9\",\n",
      "      \"44\": \"vertebrae t8\",\n",
      "      \"45\": \"vertebrae t7\",\n",
      "      \"46\": \"vertebrae t6\",\n",
      "      \"47\": \"vertebrae t5\",\n",
      "      \"48\": \"vertebrae t4\",\n",
      "      \"49\": \"vertebrae t3\",\n",
      "      \"5\": \"right kidney\",\n",
      "      \"50\": \"vertebrae t2\",\n",
      "      \"51\": \"vertebrae t1\",\n",
      "      \"52\": \"vertebrae c7\",\n",
      "      \"53\": \"vertebrae c6\",\n",
      "      \"54\": \"vertebrae c5\",\n",
      "      \"55\": \"vertebrae c4\",\n",
      "      \"56\": \"vertebrae c3\",\n",
      "      \"57\": \"vertebrae c2\",\n",
      "      \"58\": \"vertebrae c1\",\n",
      "      \"59\": \"trachea\",\n",
      "      \"6\": \"aorta\",\n",
      "      \"60\": \"heart myocardium\",\n",
      "      \"61\": \"left heart atrium\",\n",
      "      \"62\": \"left heart ventricle\",\n",
      "      \"63\": \"right heart atrium\",\n",
      "      \"64\": \"right heart ventricle\",\n",
      "      \"65\": \"pulmonary artery\",\n",
      "      \"66\": \"left iliac artery\",\n",
      "      \"67\": \"right iliac artery\",\n",
      "      \"68\": \"left iliac vena\",\n",
      "      \"69\": \"right iliac vena\",\n",
      "      \"7\": \"inferior vena cava\",\n",
      "      \"70\": \"colon\",\n",
      "      \"71\": \"left rib 1\",\n",
      "      \"72\": \"left rib 2\",\n",
      "      \"73\": \"left rib 3\",\n",
      "      \"74\": \"left rib 4\",\n",
      "      \"75\": \"left rib 5\",\n",
      "      \"76\": \"left rib 6\",\n",
      "      \"77\": \"left rib 7\",\n",
      "      \"78\": \"left rib 8\",\n",
      "      \"79\": \"left rib 9\",\n",
      "      \"8\": \"right adrenal gland\",\n",
      "      \"80\": \"left rib 10\",\n",
      "      \"81\": \"left rib 11\",\n",
      "      \"82\": \"left rib 12\",\n",
      "      \"83\": \"right rib 1\",\n",
      "      \"84\": \"right rib 2\",\n",
      "      \"85\": \"right rib 3\",\n",
      "      \"86\": \"right rib 4\",\n",
      "      \"87\": \"right rib 5\",\n",
      "      \"88\": \"right rib 6\",\n",
      "      \"89\": \"right rib 7\",\n",
      "      \"9\": \"left adrenal gland\",\n",
      "      \"90\": \"right rib 8\",\n",
      "      \"91\": \"right rib 9\",\n",
      "      \"92\": \"right rib 10\",\n",
      "      \"93\": \"right rib 11\",\n",
      "      \"94\": \"right rib 12\",\n",
      "      \"95\": \"left humerus\",\n",
      "      \"96\": \"right humerus\",\n",
      "      \"97\": \"left scapula\",\n",
      "      \"98\": \"right scapula\",\n",
      "      \"99\": \"left clavicula\"\n",
      "    }\n",
      "  },\n",
      "  \"name\": \"my_vista\",\n",
      "  \"network_arch\": \"monai_vista3d\",\n",
      "  \"ngc_path\": \"\",\n",
      "  \"ptm\": [\n",
      "    \"543a531e-4533-4444-9664-f315a1e20645\"\n",
      "  ],\n",
      "  \"public\": false,\n",
      "  \"read_only\": false,\n",
      "  \"realtime_infer\": true,\n",
      "  \"realtime_infer_request_timeout\": 60,\n",
      "  \"train_datasets\": [\n",
      "    \"62ed9367-56df-4813-b85a-b78b5e897096\"\n",
      "  ],\n",
      "  \"type\": \"monai\",\n",
      "  \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# API Endpoint and Credentials\n",
    "monai_cloud_api = \"<MONAI Cloud API URL>\"\n",
    "api_url = f\"{monai_cloud_api}/api/v1\"\n",
    "ngc_api_key = \"<NGC API Key>\"\n",
    "\n",
    "# NGC UID \n",
    "response = requests.get(f\"{api_url}/login/{ngc_api_key}\")\n",
    "uid = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{api_url}/user/{uid}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Dicom Server\n",
    "dicom_web_endpoint = \"<DICOMWeb address>\" # For example \"http://127.0.0.1:8042/dicom-web\".\n",
    "dicom_client_id = \"<DICOMWeb user ID>\"    # If Authentication is enabled, then provide username\n",
    "dicom_client_secret = \"<DICOMWeb secret>\" # If Authentication is enabled, then provide password\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow =False\n",
    "mlflow_server_address = \"\" # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\" # For example \"my_experiment\"\n",
    "\n",
    "data = {\n",
    "    \"name\": \"mydataset\",\n",
    "    \"description\":\"a demo dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"location\": f\"{dicom_web_endpoint}\",\n",
    "    \"client_id\": f\"{dicom_client_id}\",\n",
    "    \"client_secret\": f\"{dicom_client_secret}\",\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    res = response.json()\n",
    "    dataset_id = res[\"id\"]\n",
    "    print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)\n",
    "    \n",
    "endpoint = f\"{base_url}/model\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    res = response.json()\n",
    "\n",
    "    # Vista PTM\n",
    "    ptm_vista = [p for p in res if p[\"network_arch\"] == \"monai_vista3d\" and not len(p[\"ptm\"])][0][\"id\"]\n",
    "    print(f\"PTM ID for VISTA Model: {ptm_vista}\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)\n",
    "    \n",
    "data = {\n",
    "  \"name\": \"my_vista\",\n",
    "  \"description\": \"based on vista\",\n",
    "  \"network_arch\": \"monai_vista3d\",\n",
    "  \"ptm\": [ ptm_vista ],\n",
    "  \"inference_dataset\": dataset_id,\n",
    "  \"eval_dataset\": dataset_id,\n",
    "  \"train_datasets\": [ dataset_id ],\n",
    "  \"realtime_infer\": True, # Auto loads model and enables real-time inference\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/model\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    res = response.json()\n",
    "    model_id = res[\"id\"]\n",
    "    model_network = res[\"network_arch\"]\n",
    "    print(\"Model creation succeeded with model ID:\", model_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2995b-e4d7-4810-bd8a-0764326bcd7d",
   "metadata": {},
   "source": [
    "## Configuring Annotation and Continuous Learning Parameters\n",
    "\n",
    "Continuous learning is the backbone of keeping our models accurate and up-to-date. As new data is annotated, the model has the potential to learn and adapt. However, to kick off this process, we need to specify certain parameters that inform the system how and when to refine the model.\n",
    "\n",
    "With this job, the model will be fine tuned with new labeled samples after several notifications. A fine tuned model can generate a better annotation results, therefore improving the annotation efficiency.\n",
    "\n",
    "*If you prefer to only annotate data without the continuous learning process, you can simply skip this step. You can still use the annotation tools and workflows outlined in the upcoming sections independently.*\n",
    "\n",
    "### API Call for Continuous Learning Job\n",
    "\n",
    "Below, we provide the API call needed to create a continuous learning job for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1af77a-c109-48ea-bab1-784bc160542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job creation succeeded with job ID:  80a63542-5c80-43fe-9b79-f441fafa84e4\n"
     ]
    }
   ],
   "source": [
    "train_spec = {\n",
    "    \"epochs\": 10,\n",
    "    \"val_interval\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"train#handlers#-1#artifacts\": None\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "    \"round_size\": 2,  # round_size: number of images to annotate in each round\n",
    "    \"stop_criteria\": {\n",
    "        \"max_rounds\": 2,\n",
    "        \"key_metric\": 0.9,\n",
    "    },\n",
    "    \"train_spec\": train_spec,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/annotation\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    cl_job_id = response.json()[0]\n",
    "    print(\"Job creation succeeded with job ID: \", cl_job_id)\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70528bc7-2af2-4965-b0da-5af6d97aa890",
   "metadata": {},
   "source": [
    "**Parameter Details**:\n",
    "- `round_size`: Specifies how many new annotations are needed to trigger a new fine-tuning round for the model.\n",
    "- `stop_criteria`: Criteria to decide when the continuous learning job should cease. \n",
    "    - `max_rounds`: Determines the maximum rounds the job should run.\n",
    "    - `key_metric`: (Optional) If specified, the job will keep running until the designated evaluation metric reaches the value set.\n",
    "- `train_spec`: Overrides certain parameters in the model for this particular training. If you have an MLflow server set up, you can add its  parameters under tracking to enable logging metrics with MLflow.\n",
    "\n",
    "#### Check Job Status\n",
    "\n",
    "Ensure the continuous learning job is up and running as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e64d92-d346-4071-9871-5b9b0f9287f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Learning/Annotation Job status:  Running\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"{base_url}/model/{model_id}/job/{cl_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Continuous Learning/Annotation Job status: \", response.json()[\"status\"])\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861fe31-54df-4d8e-95f4-5414d2a1f1d5",
   "metadata": {},
   "source": [
    "### Using MLflow to Monitor Metrics\n",
    "\n",
    "If you've set up MLflow and included the relevant parameters in your continuous learning job, you can actively monitor the training metrics through the platform. This is invaluable for gauging the performance of your model in real-time and making timely interventions when necessary.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33996ee-34ed-4815-8075-b652242ab007",
   "metadata": {},
   "source": [
    "## VISTA Workflows\n",
    "\n",
    "Deep-dive into specific workflows that allow refined interaction with the model:\n",
    "\n",
    "1. **Segment All Classes**: Users can analyze an entire image without specific prompts, offering a comprehensive overview.\n",
    "2. **Using Class Prompts**: Users direct the model's focus towards one or more specific classes. Class-based segmentation can enable a specialized focus on a particular disease/organ.\n",
    "3. **Using Point Prompts**: Users specify a sequence of background and foreground clicks to guide the modelâ€™s focus, particularly when used together with class prompts.\n",
    "\n",
    "These workflows also integrate seamlessly with the OHIF Plugin for an enhanced visual experience, we'll walk through the OHIF experience below along with the accompanying API call used in the background.\n",
    "\n",
    "### Using Segment Everything\n",
    "By default, the VISTA-3D Model provides 118 classes and using the Auto Segmentation panel, you can run inferencing use all available classes.\n",
    "\n",
    "**Steps**\n",
    "1. Click the `run` button under the `Auto Segmentation panel` to obtain the segmentation mask for all classes.\n",
    "\n",
    "![Inference Auto Segmentation](./end2end_pic/inference_as.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c39f3a-4b0e-4336-b8eb-52292c2a177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {}\n",
    "image_id = \"<Image ID from OHIF>\"\n",
    "\n",
    "data = {\n",
    "    \"image\": image_id,\n",
    "    \"bundle_params\": bundle_params,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/inference\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Inference Succesful.  Label is returned\")\n",
    "    print(response.headers)\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b89e-0afe-4828-af51-50478649a188",
   "metadata": {},
   "source": [
    "### Using Class Prompts\n",
    "Instead of using all 118 labels, you can select a few labels that you're interested in and run inference only on those classes.  If you're using a customize version of VISTA-3D as referenced in our [Dataset Creation and Model Selection](./Dataset-Creation-and-Model-Selection.ipynb) notebook, you'll see only the classes you created with the model listed in this section.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Class Prompts` panel.\n",
    " 2. Select classes that you want to inference with class prompts.\n",
    " 3. Click the `Run` button to get the inference result.\n",
    "\n",
    "![Inference Point Prompts](./end2end_pic/inference_class_prompts.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    "![Inference Point Prompts Result](./end2end_pic/inference_class_prompts_res.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f37499-964f-40be-b08a-9f19858084de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"label_prompt\": [1, 2, 3, 4, 5], # Whichever classes were selected\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"image\": image_id,\n",
    "    \"bundle_params\": bundle_params,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/inference\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Inference Succesful.  Label is returned\")\n",
    "    print(response.headers)\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b3c06-253d-4ad2-bb4c-bde506bdbf15",
   "metadata": {},
   "source": [
    "### Using Point Prompts\n",
    "Last, instead of using only class prompts, you can use point+class prompts.  This allows you to add points to the indicated classes to help guide the model and refine your segmentation using an interactive workflow.\n",
    "\n",
    "**Steps**\n",
    " 1. Click the `Point Prompts` panel.\n",
    " 2. Select a class that you want to inference with point prompts.\n",
    " 3. Add some point to the image where you want to get the mask by clicking.\n",
    " 4. Click the `Run` button to get the inference result.\n",
    "\n",
    " ![Inference Point Prompts](./end2end_pic/inference_point.png)\n",
    "\n",
    "After a few seconds, you will see the inference result.\n",
    "\n",
    " ![Inference Point Prompts Result](./end2end_pic/inference_point_res.png)\n",
    "\n",
    "If you want to clear some points, you can either clear specific class points or clear all points by clicking the `Clear Points` or `Clear All Points` button.\n",
    "\n",
    "![Clear Points](./end2end_pic/clearpoints.png)\n",
    "\n",
    "The associated API call run when you click the `Run` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc775db-252a-4fea-adcd-12d073c35403",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_params = {\n",
    "    \"points\": [[20,20,20], [20, 40, 60]],\n",
    "    \"point_labels\": [2, 2],\n",
    "    \"label_prompt\": [2],\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"bundle_params\": bundle_params\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/inference\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Inference Succesful.  Label is returned\")\n",
    "    print(response.headers)\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1881271-8226-489a-be1b-951c8a1defc5",
   "metadata": {},
   "source": [
    "## Annotation Workflow\n",
    "\n",
    "Annotating medical images efficiently and precisely is a multi-step process. Here's a breakdown of the typical workflow you'd employ when using NVIDIA MONAI Cloud APIs and OHIF. We'll cover any relevant APIs not already covered as we walk through the workflow.\n",
    "\n",
    "`Load Image` --> `Run Inference` --> `Annotate/Fix Annotation` --> `Save /Notify` --> `Repeat`\n",
    "\n",
    "### 1. **Load Image**\n",
    "\n",
    "Begin by loading the desired medical image that you wish to annotate. If you're using OHIF, you'll see the study list and can select a patient the annotate.  Make sure to use the `MONAI Service` to load the NVIDIA MONAI Cloud API plugin.\n",
    "\n",
    "![Select an image](end2end_pic/selectanimage.png)\n",
    "\n",
    "If you're using the API directly, you can use the `nextimage` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdedb83-8f70-409e-a162-c13fd183bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}/job/nextimage\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    res = response.json()\n",
    "    image_id = res[\"image\"]\n",
    "    print(f\"Recommended Image to annotate: {image_id}\")\n",
    "    print(json.dumps(res, indent=2))\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a136bb2-fbe0-4f31-8bed-475dddaac959",
   "metadata": {},
   "source": [
    "### 2. **Run Inferencing Using Selected Method**\n",
    "\n",
    "Choose one of the inferencing methods discussed above:\n",
    "\n",
    "1. **Segment All Classes**\n",
    "2. **Using Class Prompts**\n",
    "3. **Using Point Prompts**\n",
    "\n",
    "Once you've picked your preferred method, run the inference to get an initial annotation.\n",
    "\n",
    "![allclass](./end2end_pic/allclassohif.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d341faa-6025-48fb-b6e9-5f1cb85d4428",
   "metadata": {},
   "source": [
    "### 3. **Annotate / Refine Annotations**\n",
    "\n",
    "With the initial mask in place, you might notice areas that require manual tweaking. Use the provided annotation tools to:\n",
    "\n",
    "- Refine boundaries\n",
    "- Add or remove regions\n",
    "\n",
    "This step ensures that your annotations are as accurate as possible.\n",
    "\n",
    "**Steps**\n",
    "1. Click the Segmentation button.\n",
    "2. Select a class of segmentation that needs to be updated.\n",
    "3. Select a segmentation tool.\n",
    "4. Update the segmentation with this tool.\n",
    "\n",
    "![Annotate](./end2end_pic/annotate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4ac51-3f01-4876-a1c6-f9c15c5baefd",
   "metadata": {},
   "source": [
    "### 4. **Save and Notify the Server**\n",
    "\n",
    "Once you're satisfied with your annotations, the first step is to save the annotated image, ensuring that your work is captured. This will write back the image using the DICOMWeb protocal back to your datastore.\n",
    "\n",
    "![Save Label](./end2end_pic/savelabel.png)\n",
    "\n",
    "Next, notify the server that an image has been annotated. This step is crucial for continuous learning. The system will take note of the new annotations and after the indicated number of annotated images it will use them to improve the model over time.\n",
    "\n",
    "![Notify](end2end_pic/notify.png)\n",
    "\n",
    "The associated API call run when you click the `Notify Server` button is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45e145-c4e0-4b27-81e6-43e9121fa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After uploading a DICOM Seg into DICOM Web\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}/job/notify\"\n",
    "label_id = \"<series_id_1>\"\n",
    "data = {\n",
    "    \"added\": {\n",
    "        \"image\": image_id,\n",
    "        \"label\": label_id,\n",
    "    },\n",
    "    \"updated\": [],\n",
    "    \"removed\": [],\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    print(\"Notified.\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218b343-5897-4b3d-8ddb-f0e21748f1b6",
   "metadata": {},
   "source": [
    "### 5. **Repeat**\n",
    "\n",
    "Continue the process for all the images in your dataset. With each iteration, not only do you expand your annotated dataset, but you also contribute to the model's learning, making future annotations even more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c45c9-cb76-4890-ac40-7c4d8c3697fc",
   "metadata": {},
   "source": [
    "## Stopping a Continuous Learning Job\n",
    "\n",
    "As your model refines itself over time using continuous learning, there might come a point where you need to halt the ongoing CL job. Whether you're satisfied with the model's performance or have other reasons, here's how you can stop the CL job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b4d8-4353-408b-9c6a-99d159facffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually stop the CL job. No need to execute this cell if the job has reached the stop criteria.\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{cl_job_id}/cancel\"\n",
    "response = requests.post(endpoint, headers=headers)\n",
    "print(response.json())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b45c32-9d44-4bd2-896e-884425e0447c",
   "metadata": {},
   "source": [
    "## Exporting the Model\n",
    "\n",
    "After you've trained the model, you might want to export it for various purposes.  Here's how you can accomplish that using the following APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all jobs and pick one job that meets your requirement.\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "count = 0\n",
    "if response.status_code == 200:\n",
    "    job_metas = response.json()\n",
    "    for job_meta in job_metas:\n",
    "        if job_meta[\"id\"] == cl_job_id:\n",
    "            print(\"Continuous Learning Job status: \", job_meta[\"status\"])\n",
    "        else:\n",
    "            count += 1\n",
    "            print(f\"Training Job #{count} status: \", job_meta[\"status\"])\n",
    "            if job_meta[\"status\"] == \"Done\":\n",
    "                print(f\"Training Job #{count} with ID {job_meta['id']} metric: \", job_meta[\"result\"][\"key_metric\"])\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5822ee2-42f5-490a-aeca-a975504392a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a job id from the last cell output.\n",
    "download_job_id = \"<job ID you want to download>\"\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{download_job_id}/download\"\n",
    "response = requests.get(endpoint, data=json.dumps({\"export_type\": \"monai_bundle\"}), headers=headers)\n",
    "if response.status_code == 200:\n",
    "    with open(f\"{download_job_id}.tar.gz\", \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    print(\"Downloaded!\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59312363-d36e-440f-9044-185882c19517",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Remember, the NVIDIA MONAI Cloud APIs are designed to streamline this process, making it intuitive and efficient. As you work through these steps, the platform aids you, ensuring that you can focus on the quality of annotations while the technical details are handled seamlessly in the background.\n",
    "\n",
    "Make the most of continuous learning and annotation with the NVIDIA MONAI Cloud APIs. This iterative refinement paves the way to excellence in medical imaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MONAI Fix",
   "language": "python",
   "name": "monai_fix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
