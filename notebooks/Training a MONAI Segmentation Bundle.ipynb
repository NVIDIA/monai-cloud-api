{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a MONAI Segmentation Bundle\n",
    "\n",
    "This tutorial demonstrates how to train a typical MONAI segmentation bundle on the NVIDIA DGX Cloud. It focuses on utilizing the powerful capabilities of DGX systems for medical imaging applications, specifically using a MONAI Vista bundle.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Training%20a%20MONAI%20Segmentation%20Bundle.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Setup\n",
    "- Datasets Creation\n",
    "- Experiment Creation\n",
    "- Run a Batch Training Job\n",
    "- Batch Inference\n",
    "- Monitoring Job Status and Logging\n",
    "- Clean Up\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to train a MONAI segmentation bundle on the NVIDIA DGX Cloud. It focuses on utilizing the powerful capabilities of DGX systems for medical imaging applications, specifically using a MONAI Vista 3D bundle.\n",
    "\n",
    "### What You Can Expect to Learn\n",
    "\n",
    "This tutorial is designed to guide you through the process of training a MONAI segmentation bundle using the NVIDIA DGX Cloud. Throughout this guide, you will run batch training and inference jobs, as well as monitoring their progress effectively. By the end of this tutorial, you will have the models and inference results on your remote cloud storage bucket.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import requests\" || pip install -q \"requests\"\n",
    "!python -c \"import nibabel\" || pip install -q \"nibabel\"\n",
    "!python -c \"import matplotlib\" || pip install -q \"matplotlib\"\n",
    "!python -c \"import libcloud\" || pip install -q \"apache-libcloud\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import requests\n",
    "from libcloud.storage.providers import get_driver\n",
    "from libcloud.storage.types import Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Provided the following parameters to start this notebook.\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "\n",
    "# The cloud storage type used in this notebook. Currently only support `aws` and `azure`.\n",
    "cloud_type = \"azure\"  # cloud storage provider: aws or azure\n",
    "cloud_account = \"account_name\"  # if cloud_type == \"aws\"  should be \"access_key\"\n",
    "cloud_secret = \"access_key\"  # if cloud_type == \"aws\" should be \"secret_key\"\n",
    "\n",
    "# Cloud storage credentials. Needed for storing the data and results of the experiments.\n",
    "access_id = \"<user name for the remote storage object>\"  # Please fill it with the actual Access ID\n",
    "access_secret = \"<secret for the remote storage object>\"  # Please fill it with the actual Access Secret\n",
    "\n",
    "# Dataset Cloud Storage URL. This is the cloud storage where the training and validation dataset is stored.\n",
    "train_manifest_url = \"<train manifest url>\"\n",
    "val_manifest_url = \"<validation manifest url>\"\n",
    "\n",
    "# Experiment Cloud Storage. This is the storage where your jobs and experiments data will be stored.\n",
    "cs_bucket = \"<bucket or container name to push experiment job data to>\"  # Please fill it with the actual bucket name\n",
    "\n",
    "# Inference workflow parameters. If the inference is needed after/before training a model, set the do_inference parameter to True.\n",
    "do_inference = True\n",
    "do_inference_on_trained_model = True\n",
    "if do_inference:\n",
    "    inference_manifest_url = \"<inference manifest url>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login into NGC and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", json={\"ngc_api_key\": ngc_api_key})\n",
    "response.raise_for_status()\n",
    "assert \"user_id\" in response.json(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json(), \"token is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "ngc_org = \"iasixjqzw1hj\"  # This is the default org for MONAI users. Please select the correct org if you are not using the default one.\n",
    "base_url = f\"{api_url}/orgs/{ngc_org}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "print(\"API Calls will be forwarded to\", base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "### Create the Training Dataset and the Validation Dataset\n",
    "\n",
    "Define and create your training and validation datasets using the MONAI Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_seg_train\",\n",
    "    \"description\": \"Remote storage object dataset for training\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": train_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create train dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "train_dataset_id = res[\"id\"]\n",
    "print(\"Training dataset creation succeeded with dataset ID:\", train_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "# Validation dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_seg_val\",\n",
    "    \"description\":\"Remote storage object dataset for validation\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": val_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create val dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "val_dataset_id = res[\"id\"]\n",
    "print(\"Validation dataset creation succeeded with dataset ID:\", val_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the inference dataset (optional)\n",
    "\n",
    "Define and create your inference dataset using the MONAI Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_inference:\n",
    "    # Inference dataset\n",
    "    data = {\n",
    "        \"name\": \"MONAI_seg_infer\",\n",
    "        \"description\": \"Remote storage object dataset for training\",\n",
    "        \"type\": \"semantic_segmentation\",\n",
    "        \"format\": \"monai\",\n",
    "        \"client_url\": inference_manifest_url,\n",
    "        \"client_id\": access_id,\n",
    "        \"client_secret\": access_secret,\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    assert response.status_code == 201, f\"Create inference dataset failed, got {response.json()}.\"\n",
    "    res = response.json()\n",
    "    inference_dataset_id = res[\"id\"]\n",
    "    print(\"Inference dataset creation succeeded with dataset ID:\", inference_dataset_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Creation\n",
    "\n",
    "Create a MONAI segmentation experiment, specifying the necessary parameters and datasets. In this tutorial, we will use the vista3d bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Base Experiments\n",
    "\n",
    "#### Find the base experiment for VISTA-3D and MONAI Annotation (DeepEdit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List base experiments failed, got {response.text}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "vista3d_base_exps = [p for p in res[\"experiments\"] if p[\"network_arch\"] == \"monai_vista3d\"]\n",
    "assert len(vista3d_base_exps) > 0, \"No base experiment found for VISTA 3D bundle\"\n",
    "print(\"List of available base experiments for VISTA 3D bundle:\")\n",
    "for exp in vista3d_base_exps:\n",
    "    print(f\"  {exp['id']}: {exp['name']} v{exp['version']}\")\n",
    "base_experiment = sorted(vista3d_base_exps, key=lambda x: x[\"version\"])[-1]  # Take the latest version\n",
    "version_vista = base_experiment[\"version\"]\n",
    "base_exp_vista = base_experiment[\"id\"]\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(f\"Base experiment ID for '{base_experiment['name']}' v{base_experiment['version']}: {base_exp_vista}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "deepedit_base_exps = [p for p in res[\"experiments\"] if p[\"network_arch\"] == \"monai_annotation\" and not p[\"base_experiment\"]]\n",
    "assert len(deepedit_base_exps) > 0, \"No base experiment found for MONAI Annotation (DeepEdit) bundle\"\n",
    "print(\"List of available base experiments for MONAI Annotation (DeepEdit) bundle:\")\n",
    "for exp in deepedit_base_exps:\n",
    "    print(f\"  {exp['id']}: {exp['name']} v{exp['version']}\")\n",
    "base_experiment = sorted(deepedit_base_exps, key=lambda x: x[\"version\"])[-1]  # Take the latest version\n",
    "version_annotation = base_experiment[\"version\"]\n",
    "base_exp_annotation = base_experiment[\"id\"]\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(f\"Base experiment ID for '{base_experiment['name']}' v{base_experiment['version']}: {base_exp_vista}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create workspace for experiments to upload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_data = {\n",
    "    \"name\": \"Azure workspace info\",  # A representative name for this cloud info\n",
    "    \"cloud_type\": cloud_type,\n",
    "    \"cloud_specific_details\": {\n",
    "        \"cloud_bucket_name\": cs_bucket,\n",
    "        cloud_account: access_id,\n",
    "        cloud_secret: access_secret,\n",
    "    },\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "response = requests.post(endpoint, json=cloud_data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create workspace failed, got {response.text}.\"\n",
    "workspace_id = response.json()[\"id\"]\n",
    "print(\"Workspace creation succeeded with workspace ID: \", workspace_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "Set up and create your segmentation experiment based on the retrieved information. Run a batch training job with the created experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"my_vista\",\n",
    "    \"description\": \"based on vista\",\n",
    "    \"network_arch\": \"monai_vista3d\",\n",
    "    \"type\": \"medical\",\n",
    "    \"base_experiment\": [ base_exp_vista ],\n",
    "    \"eval_dataset\": val_dataset_id,\n",
    "    \"train_datasets\": [ train_dataset_id ],\n",
    "    \"workspace\": workspace_id,\n",
    "}\n",
    "\n",
    "if do_inference:\n",
    "    data.update({\"inference_dataset\": inference_dataset_id})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Batch Training Job\n",
    "\n",
    "Configure and initiate a batch training job on the DGX cloud, specifying the number of epochs and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"train#trainer#max_epochs\": 2,  # the key to override epochs depend on the MONAI bundle\n",
    "    \"val_interval\": 1,\n",
    "}\n",
    "\n",
    "data = {\"name\": \"my_monai_segmentation\", \"action\": \"train\", \"specs\": train_spec}\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx train job failed, got {response.json()}.\"\n",
    "train_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID: \", train_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status and Logging\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(endpoint, headers, timeout=1800, interval=5, target_status=\"Done\"):\n",
    "    \"\"\"Helper function to wait for job to reach target status.\"\"\"\n",
    "    expected = [\"Pending\", \"Running\", \"Done\"]\n",
    "    assert target_status in expected, f\"Invalid target status: {target_status}\"\n",
    "    status_before_target = expected[:expected.index(target_status)]\n",
    "    start_time = time.time()\n",
    "    print(f\"Waiting for job to reach state {target_status} ...\")\n",
    "    status = None\n",
    "    while True:\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"\\nJob timeout after {timeout} seconds with last status {status_new}.\")\n",
    "            break\n",
    "        elif status_new not in status_before_target:\n",
    "            assert status_new == target_status, f\"Job failed with status: {status_new}\"\n",
    "            print(f\"\\nJob reached target status: {status_new}\")\n",
    "            break\n",
    "        print(f\"\\n{status_new}\", end=\"\", flush=True) if status_new != status else print(\".\", end=\"\", flush=True)\n",
    "        status = status_new\n",
    "        time.sleep(interval)\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, timeout=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Log Download\n",
    "\n",
    "Access and download job logs to troubleshoot or assess performance. The job log is available when the status of the job is `RUNNING`, `Error` or `Done`. This API is available for all kinds of jobs.\n",
    "\n",
    "Please note that the job log will not be immediately available after the status turns to `RUNNING` since it takes a while to prepare the environment for the running job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "# In order to download the job, the training process should be finished\n",
    "job_meta = response.json()\n",
    "status = job_meta[\"status\"].title()\n",
    "if status in [\"Running\", \"Done\", \"Error\"]:\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}/logs\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job logs, got {response.text}.\"\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Job status: {status}, logs are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the key metric in the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status == \"Done\":\n",
    "    epoch = job_meta[\"result\"][\"epoch\"]\n",
    "    key_metric = job_meta[\"result\"][\"key_metric\"]\n",
    "    print(f\"Training Job {train_job_id} completed with key_metric {key_metric} achieved in the Epoch {epoch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the job results (checkpoint, scripts, logs, etc.)\n",
    "\n",
    "You'll find the results in the cloud storage bucket you specified when creating the experiment. The results will include the model checkpoints, scripts, logs, and other relevant data.\n",
    "\n",
    "The path to the results will be in the following format:\n",
    "\n",
    "```python\n",
    "f\"{bucket_name}/results/{job_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference (Optional)\n",
    "\n",
    "You can do the batch inference action inside a experiment. There are two ways to do inference after created an experiment and set up the pretrained model:\n",
    "1. Do batch inference with the pretrained model.\n",
    "1. Do batch inference with a fine-tuned model.\n",
    "\n",
    "For the first way, you can directly run the batch inference action before a training job without specified job ID. Then the inference will be performed with the pretrained model.\n",
    "For the second way, you need to run a batch training job first and set the training job id to the input parameters. By doing this, the inference is performed based on the fine-tuned model inside the given training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference with Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_inference and not do_inference_on_trained_model:    \n",
    "    inference_spec = {}\n",
    "    data = {\"name\": \"vista3d_infer\", \"action\": \"batchinfer\", \"specs\": inference_spec}\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "    assert response.status_code == 201, f\"Run batch inference job failed, got {response.json()}.\"\n",
    "    infer_job_id = response.json()\n",
    "    print(\"Job creation succeeded with job ID: \", infer_job_id)\n",
    "\n",
    "    # During the Job is Running \n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    for k, v in response.json().items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference with Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_inference and do_inference_on_trained_model:    \n",
    "    inference_spec = {\"train_job_id\": train_job_id}\n",
    "    data = {\"name\": \"vista3d_infer\", \"action\": \"batchinfer\", \"specs\": inference_spec}\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "    assert response.status_code == 201, f\"Run batch inference job failed, got {response.json()}.\"\n",
    "    infer_job_id = response.json()\n",
    "    print(\"Job creation succeeded with job ID: \", infer_job_id)\n",
    "\n",
    "    # During the Job is Running \n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    for k, v in response.json().items():\n",
    "        print(f\"{k}: {v}\")\n",
    "        \n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Vsiualize the Inference Images\n",
    "\n",
    "Download the predicted masks from the cloud storage to your local machine for further analysis, visualization, and integration into medical imaging applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = f\"results/{infer_job_id}/vista3d_segmentation_v{version_vista}/eval\"\n",
    "\n",
    "if cloud_type == \"aws\":\n",
    "    cs_driver = get_driver(Provider.S3)\n",
    "elif cloud_type == \"azure\":\n",
    "    cs_driver = get_driver(Provider.AZURE_BLOBS)\n",
    "\n",
    "driver = cs_driver(access_id, access_secret, region=\"us-west-1\")\n",
    "container = driver.get_container(container_name=cs_bucket)\n",
    "\n",
    "file_objects = driver.list_container_objects(container=container, ex_prefix=folder)\n",
    "for obj in file_objects:\n",
    "    local_destination = obj.name\n",
    "    print(\"Downloading object: %s\" % obj.name)\n",
    "    obj.download(os.path.basename(obj.name), overwrite_existing=True)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "# walk through the downloaded files\n",
    "infer_files = sorted([f for f in os.listdir() if f.endswith(\".nii.gz\")])\n",
    "\n",
    "# plot the midplane of the first file\n",
    "data = nib.load(infer_files[0]).get_fdata()\n",
    "plt.imshow(data[:, :, data.shape[2] // 2], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment and datasets to clean up resources once all jobs are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "for job in response.json()[\"jobs\"]:\n",
    "    job_id = job[\"id\"]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    # If the job is not done, need to cancel it first\n",
    "    if response.json()[\"status\"] != \"Done\":\n",
    "        endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "        response = requests.post(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Cancel job failed, got {response.json()}.\"\n",
    "        print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete train dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "# validation dataset\n",
    "endpoint = f\"{base_url}/datasets/{val_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete val dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "if do_inference:\n",
    "    # inference dataset\n",
    "    endpoint = f\"{base_url}/datasets/{inference_dataset_id}\"\n",
    "    response = requests.delete(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Delete inference dataset failed, got {response.json()}.\"\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/workspaces/{workspace_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete workspace failed, got {response.text}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching this pivotal milestone! With your dataset created and experiment selected, you're now fully equipped to leverage training features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
