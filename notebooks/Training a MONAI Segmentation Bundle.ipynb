{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a MONAI Segmentation Bundle\n",
    "\n",
    "This tutorial is designed to show how to train a typical MONAI segmentation bundle onthe NVIDIA DGX Cloud, focusing on leveraging the powerful capabilities of DGX systems for medical imaging applications. We will use a MONAI vista bundle to showcase this example.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Login with NGC Key](#Login-with-NGC-Key)\n",
    "2. [Datasets Creation](#Datasets-Creation)\n",
    "3. [Experiment Creation](#Experiment-Creation)\n",
    "4. [Monitoring Job Status](#Monitoring-Job-Status)\n",
    "5. [Bundle Download](#Bundle-Download)\n",
    "6. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Setup'></a>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Provided the following parameters to start this notebook.\n",
    "host_url = \"<monai service API address>\"\n",
    "ngc_api_key = \"<your ngc keys>\"\n",
    "# Object storage info\n",
    "access_id = \"<user name for the object storage>\"\n",
    "access_secret = \"<secret for the object storage>\"\n",
    "train_manifest_url = \"<train manifest url>\"\n",
    "val_manifest_url = \"<validation manifest url>\"\n",
    "# training parameters\n",
    "train_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login with NGC Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "print(response.status_code)\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Creation\n",
    "\n",
    "### **1. Remote Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports a range of other cloud storage solutions, including Azure Blob Storage and Amazon S3, providing you with the flexibility to choose the service that best fits your project's needs. Below is an example of Azure:\n",
    "\n",
    "**Steps:**\n",
    "1. [Creating a Storage Account and Container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction)\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. [Container URL](https://learn.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata)\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "#### Obtaining Credentials\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the [Access keys](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "#### Creating a Manifest JSON File\n",
    "\n",
    "In the root of your Azure container, create a manifest JSON file to keep track of your datasets. The file format is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"path\": [\"path/to/your/label_1\"],\n",
    "                \"id\": \"unique-uuid-2\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "- Each dataset (training, testing, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" and \"label\" fields\n",
    "- Each \"image\"/\"label\" field should contain \"path\" field, which is the list of relative path to the image/label files\n",
    "- Please provide the \"id\" field of the \"image\"/\"label\", if there is not one please provide a random uuid  \n",
    "\n",
    "\n",
    "After preparing your dataset, please modify the following variables in [Setup](#Setup):\n",
    "\n",
    "```python\n",
    "access_id = ...\n",
    "access_secret = ...\n",
    "train_manifest_url = ...\n",
    "val_manifest_url = ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create the training dataset and the validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_seg_train\",\n",
    "    \"description\":\"Object storage dataset for training\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": train_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "data=json.dumps(data)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "print(endpoint)\n",
    "print(headers)\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response.json())\n",
    "\n",
    "assert response.status_code == 201, f\"Create train dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "train_dataset_id = res[\"id\"]\n",
    "print(\"Train dataset creation succeeded with dataset ID:\", train_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "# Validation dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_seg_val\",\n",
    "    \"description\":\"Object storage dataset for validation\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": val_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "data=json.dumps(data)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "print(endpoint)\n",
    "print(headers)\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response.json())\n",
    "\n",
    "assert response.status_code == 201, f\"Create val dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "val_dataset_id = res[\"id\"]\n",
    "print(\"Validation dataset creation succeeded with dataset ID:\", val_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Creation\n",
    "\n",
    "Create an experiment based on a MONAI segmentation bundle. In this notebook, we will use the vista3d bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.List Available Base Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List Base Experiments failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "ptm_vista = [p for p in res if p[\"network_arch\"] == \"monai_vista3d\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for VISTA Experiment: {ptm_vista}\")\n",
    "\n",
    "# DeepEdit\n",
    "ptm_annotation = [p for p in res if p[\"network_arch\"] == \"monai_annotation\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for DeepEdit(Annotation) Experiment: {ptm_annotation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.Create Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"name\": \"my_vista\",\n",
    "  \"description\": \"based on vista\",\n",
    "  \"network_arch\": \"monai_vista3d\",\n",
    "  \"type\": \"medical\",\n",
    "  \"base_experiment\": [ ptm_vista ],\n",
    "  \"eval_dataset\": val_dataset_id,\n",
    "  \"train_datasets\": [ train_dataset_id ],\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "model_network = res[\"network_arch\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.Run a DGX Train Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"train#trainer#max_epochs\": train_epochs,\n",
    "    \"val_interval\": 1,\n",
    "}\n",
    "\n",
    "data = {\"action\": \"train\", \"specs\": train_spec}\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx train job failed, got {response.json()}.\"\n",
    "job_id = response.json()\n",
    "print(\"Job creation succeeded with job IDï¼š \", job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running jobs\n",
    "def wait_for_job(endpoint, headers, timeout):\n",
    "    start_time = time.time()\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    status = response.json()[\"status\"].title()\n",
    "    print(\"Waiting for job to complete...\")\n",
    "    print(status, end=\"\", flush=True)\n",
    "    while True:\n",
    "        if status not in [\"Pending\", \"Running\"]:\n",
    "            assert status == \"Done\", f\"Job failed with status: {status}\"\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if status_new != status:\n",
    "            status = status_new\n",
    "            print(f\"\\n{status}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        if time.time() - start_time > timeout:\n",
    "            assert False, f\"Job timeout after {timeout} seconds.\"\n",
    "    print(\"\\nJob completed successfully!\")\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundle Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the trained bundle from the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:download\"\n",
    "response = requests.get(endpoint, data=json.dumps({\"export_type\": \"monai_bundle\"}), headers=headers)\n",
    "assert response.status_code == 200, f\"Failed to download bundle, got {response.json()}.\"\n",
    "with open(f\"{job_id}.tar.gz\", \"wb\") as fp:\n",
    "    fp.write(response.content)\n",
    "print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment after all jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete train dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "# validation dataset\n",
    "endpoint = f\"{base_url}/datasets/{val_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete val dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
