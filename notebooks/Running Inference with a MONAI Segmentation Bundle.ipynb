{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Inference with a MONAI Segmentation Bundle\n",
    "\n",
    "This tutorial is designed to show how to run inference on the given dataset with a pretrained or fine-tuned MONAI segmentation bundle on the NVIDIA DGX Cloud, focusing on leveraging the powerful capabilities of DGX systems for medical imaging applications. We will use a `spleen_deepedit_annotation` bundle to showcase this example.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Training%20a%20MONAI%20Segmentation%20Bundle.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Login with NGC Key\n",
    "- Datasets Creation\n",
    "- Experiment Creation\n",
    "- Monitoring Job Status\n",
    "- Clean Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Setup'></a>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Provided the following parameters to start this notebook.\n",
    "host_url = \"<monai service API address>\"\n",
    "ngc_api_key = os.environ.get('MONAI_API_KEY')\n",
    "# Object storage info\n",
    "client_id = \"<user name for the object storage>\"\n",
    "client_secret = \"<secret for the object storage>\"\n",
    "inference_manifest_url = \"<inference manifest url>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login with NGC Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "print(response.status_code)\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "### **1. Remote Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports a range of other cloud storage solutions, including Azure Blob Storage, Google Cloud Storage (GCP) and Amazon S3, providing you with the flexibility to choose the service that best fits your project's needs. Below is an example of Azure:\n",
    "\n",
    "**Steps:**\n",
    "1. Creating a Storage Account and Container\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. Container URL\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "## Obtaining Credentials\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the 'Access keys' section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "## Creating a Manifest JSON File\n",
    "\n",
    "In the root of your Azure container, create a manifest JSON file to keep track of your datasets. The file format is as follows:\n",
    "\n",
    "For a segmentation task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]/[subfolder-path]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "- Each dataset (training, testing, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" fields\n",
    "- Each \"image\" field should contain \"path\" field, which is the list of relative path to the image files\n",
    "- Please provide the \"id\" field of the \"image\"/\"label\", if there is not one please provide a random uuid generated by `uuid` package\n",
    "\n",
    "After preparing your dataset, please modify the following variables in [Setup](#Setup):\n",
    "\n",
    "```python\n",
    "access_id = ...\n",
    "access_secret = ...\n",
    "train_manifest_url = ...\n",
    "val_manifest_url = ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create a dataset for inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_seg_infer\",\n",
    "    \"description\":\"Object storage dataset for training\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": inference_manifest_url,\n",
    "    \"client_id\": client_id,\n",
    "    \"client_secret\": client_secret,\n",
    "}\n",
    "data=json.dumps(data)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "print(endpoint)\n",
    "print(headers)\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response.json())\n",
    "\n",
    "assert response.status_code == 201, f\"Create inference dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "inference_dataset_id = res[\"id\"]\n",
    "print(\"Inference dataset creation succeeded with dataset ID:\", inference_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Creation\n",
    "\n",
    "Create an experiment based on a MONAI segmentation bundle. In this notebook, we will use the spleen_deepeit_annotation bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. List Available Base Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List Base Experiments failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "ptm_vista = [p for p in res if p[\"network_arch\"] == \"monai_vista3d\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for VISTA Experiment: {ptm_vista}\")\n",
    "\n",
    "# DeepEdit\n",
    "ptm_annotation = [p for p in res if p[\"network_arch\"] == \"monai_annotation\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for DeepEdit(Annotation) Experiment: {ptm_annotation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"name\": \"my_deepedit\",\n",
    "  \"description\": \"based on spleen_deepedit\",\n",
    "  \"network_arch\": \"monai_annotation\",\n",
    "  \"type\": \"medical\",\n",
    "  \"base_experiment\": [ ptm_annotation ],\n",
    "  \"inference_dataset\": inference_dataset_id,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "model_network = res[\"network_arch\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Run a DGX Inference Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_spec = {}\n",
    "data = {\"name\": \"deepedit_inference\", \"action\": \"batchinfer\", \"specs\": inference_spec}\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx inference job failed, got {response.json()}.\"\n",
    "job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID: \", job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status and Downloading Job\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state. Here's what you need to know:\n",
    "\n",
    "1. **Basic Status Overview**: The monitoring functionality in our system is designed to inform you whether your jobs are in a pending, running, done, or error state. This status update allows you to quickly assess the overall progress and detect any immediate issues that may require attention.\n",
    "\n",
    "Status interpretation:\n",
    "- \"Pending\": MONAI cloud is looking for resources and preparing the datasets. This can take quite a while, and depends on the size of the dataset.\n",
    "- \"Running\": MONAI cloud has submitted the job to the DGX. \n",
    "- \"Done\": The training is complete\n",
    "- \"Error\": There is some error in the job. User probably wants to download the job as a `.tar.gz` archive and inspect the detailed log.\n",
    "\n",
    "2. **Detailed Logging Through Download API**: For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs, model checkpoints, and data outputs, which are instrumental for troubleshooting, in-depth analysis, and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running jobs\n",
    "def wait_for_job(endpoint, headers, timeout):\n",
    "    start_time = time.time()\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    status = response.json()[\"status\"].title()\n",
    "    print(\"Waiting for job to complete...\")\n",
    "    print(status, end=\"\", flush=True)\n",
    "    while True:\n",
    "        if status not in [\"Pending\", \"Running\"]:\n",
    "            assert status == \"Done\", f\"Job failed with status: {status}\"\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if status_new != status:\n",
    "            status = status_new\n",
    "            print(f\"\\n{status}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"Job timeout after {timeout} seconds.\")\n",
    "            break\n",
    "    print(f\"\\nJob status: {status}\")\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment after all jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "# If the job is not done, need to cancel it first\n",
    "if response.json()[\"status\"] != \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Cancel job failed, got {response.json()}.\"\n",
    "    print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete inference dataset\n",
    "endpoint = f\"{base_url}/datasets/{inference_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete train dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching this pivotal milestone! With your dataset created and experiment selected, you're now fully equipped to leverage training features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
