{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a VISTA2d Bundle\n",
    "\n",
    "This tutorial demonstrates how to train VISTA2d bundle on the NVIDIA DGX Cloud. It focuses on utilizing the powerful capabilities of DGX systems for medical imaging applications, specifically using a MONAI VISTA2d bundle.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Training%20a%20VISTA2d%20Bundle.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Setup\n",
    "- Datasets Creation\n",
    "- Experiment Creation\n",
    "- Monitoring Job Status and Downloading Job\n",
    "- Clean Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Provided the following parameters to start this notebook.\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "\n",
    "# The cloud storage type used in this notebook. Currently only support `aws` and `azure`.\n",
    "cloud_type = \"azure\" # cloud storage provider: aws or azure\n",
    "cloud_account = \"account_name\" # if cloud_type == \"aws\"  should be \"access_key\"\n",
    "cloud_secret = \"access_key\" # if cloud_type == \"aws\" should be \"secret_key\"\n",
    "\n",
    "# Cloud storage credentials. Needed for storing the data and results of the experiments.\n",
    "access_id = \"<user name for the remote storage object>\"  # Please fill it with the actual Access ID\n",
    "access_secret = \"<secret for the remote storage object>\"  # Please fill it with the actual Access Secret\n",
    "\n",
    "# Dataset Cloud Storage URL. This is the cloud storage where the training and validation dataset is stored.\n",
    "train_manifest_url = \"<train manifest url>\"\n",
    "val_manifest_url = \"<validation manifest url>\"\n",
    "\n",
    "# Experiment Cloud Storage. This is the storage where your jobs and experiments data will be stored.\n",
    "cs_bucket = \"<bucket or container name to push experiment job data to>\"  # Please fill it with the actual bucket name\n",
    "\n",
    "# Inference workflow parameters. If the inference is needed after/before training a model, set the do_inference parameter to True.\n",
    "do_inference = True\n",
    "do_inference_on_trained_model = True\n",
    "if do_inference:\n",
    "    inference_manifest_url = \"<inference manifest url>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login into NGC and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", json={\"ngc_api_key\": ngc_api_key})\n",
    "response.raise_for_status()\n",
    "assert \"user_id\" in response.json(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json(), \"token is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "ngc_org = \"iasixjqzw1hj\"\n",
    "base_url = f\"{api_url}/orgs/{ngc_org}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "print(\"API Calls will be forwarded to\", base_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "### **1. Remote Storage Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports various cloud storage solutions, including Azure Blob Storage, Google Cloud Platform (GCP) and Amazon S3. This section explains how to use Azure Blob Storage to store your datasets:\n",
    "\n",
    "**Steps:**\n",
    "1. Creating a Storage Account and Container\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. Container URL\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "### **2. Obtaining Credentials**\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the 'Access keys' section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "### **3. Creating a Manifest JSON File**\n",
    "\n",
    "In the root of your Azure blob storage container, create a manifest JSON file to track your datasets. Includes paths to your images, labels and files, provide unique IDs for each. The file format is as follows:\n",
    "\n",
    "For a segmentation task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]/[subfolder-path]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"path\": [\"path/to/your/label_1\"],\n",
    "                \"id\": \"unique-uuid-2\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "For a non-segmentation task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]\",\n",
    "    \"label_key\": [\"bbox\", \"label\"],\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"bbox\": ...,\n",
    "            \"label\": ...\n",
    "        }\n",
    "        // Additional data objects should follow the same format\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "For an inference task:\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]/[subfolder-path]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "- Each dataset (training, validation, inference, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" fields\n",
    "- The data entry for training and validation should contain \"label\" fields\n",
    "- Each \"image\"/\"label\" field should contain \"path\" field, which is the list of relative path to the image/label files\n",
    "- Please provide the \"id\" field of the \"image\"/\"label\", if there is not one please provide a random uuid generated by `uuid` package\n",
    "- The `label_key` is optional, with a default of `[\"label\"]`\n",
    "\n",
    "After preparing your dataset, update the following variables in **Environment Setup** accordingly:\n",
    "\n",
    "```python\n",
    "access_id = ...\n",
    "access_secret = ...\n",
    "train_manifest_url = ...\n",
    "val_manifest_url = ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training Dataset and the Validation Dataset\n",
    "\n",
    "Define and create your training and validation datasets using the MONAI Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "# Training dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_vista2d_train\",\n",
    "    \"description\":\"Remote storage object dataset for training\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": train_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "print(response.json())\n",
    "assert response.status_code == 201, f\"Create train dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "train_dataset_id = res[\"id\"]\n",
    "print(\"Train dataset creation succeeded with dataset ID:\", train_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "# Validation dataset\n",
    "data = {\n",
    "    \"name\": \"MONAI_vista2d_val\",\n",
    "    \"description\":\"Remote storage object dataset for validation\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": val_manifest_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "print(response.json())\n",
    "\n",
    "assert response.status_code == 201, f\"Create val dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "val_dataset_id = res[\"id\"]\n",
    "print(\"Validation dataset creation succeeded with dataset ID:\", val_dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the inference dataset (optional)\n",
    "\n",
    "Define and create your inference dataset using the MONAI Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_inference:\n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    # Inference dataset\n",
    "    data = {\n",
    "        \"name\": \"MONAI_vista2d_infer\",\n",
    "        \"description\":\"Remote storage object dataset for inference\",\n",
    "        \"type\": \"semantic_segmentation\",\n",
    "        \"format\": \"monai\",\n",
    "        \"client_url\": inference_manifest_url,\n",
    "        \"client_id\": access_id,\n",
    "        \"client_secret\": access_secret,\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    print(response.json())\n",
    "\n",
    "    assert response.status_code == 201, f\"Create inference dataset failed, got {response.json()}.\"\n",
    "    res = response.json()\n",
    "    inference_dataset_id = res[\"id\"]\n",
    "    print(\"Inference dataset creation succeeded with dataset ID:\", inference_dataset_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Creation\n",
    "\n",
    "Create a MONAI segmentation experiment, specifying the necessary parameters and datasets. In this tutorial, we will use the vista2d bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Base Experiments\n",
    "\n",
    "#### Find the base experiment for VISTA-2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List Experiments failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "vista3d_base_exps = [p for p in res[\"experiments\"] if p[\"network_arch\"] == \"monai_vista2d\" and not p[\"base_experiment\"]]\n",
    "assert len(vista3d_base_exps) > 0, \"No base experiment found for VISTA 2D bundle\"\n",
    "print(\"List of available base experiments for VISTA 2D bundle:\")\n",
    "for exp in vista3d_base_exps:\n",
    "    print(f\"  {exp['id']}: {exp['name']} v{exp['version']}\")\n",
    "base_experiment = sorted(vista3d_base_exps, key=lambda x: x[\"version\"])[-1]  # Take the latest version\n",
    "vista_bundle_name = base_experiment[\"ngc_path\"].split(\"/\")[-1].replace(\":\", \"_v\")\n",
    "version = base_experiment[\"version\"]\n",
    "base_exp_vista = base_experiment[\"id\"]\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(f\"Base experiment ID for '{base_experiment['name']}' v{base_experiment['version']}: {base_exp_vista}\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "Set up and create your segmentation experiment based on the retrieved information. Run a batch training job with the created experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cloud_details = {\n",
    "    \"cloud_type\": cloud_type,\n",
    "    \"cloud_file_type\": \"folder\",  # If the file is tar.gz key in \"file\", else \"folder\"\n",
    "    \"cloud_specific_details\": {\n",
    "        \"cloud_bucket_name\": cs_bucket,  # Bucket link to save files\n",
    "        cloud_account: access_id,  # Access and Secret for Azure\n",
    "        cloud_secret: access_secret,  # Access and Secret for Azure\n",
    "    }\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"my_vista\",\n",
    "    \"description\": \"based on vista\",\n",
    "    \"network_arch\": \"monai_vista2d\",\n",
    "    \"type\": \"medical\",\n",
    "    \"base_experiment\": [ base_exp_vista ],\n",
    "    \"eval_dataset\": val_dataset_id,\n",
    "    \"train_datasets\": [ train_dataset_id ],\n",
    "    \"cloud_details\": experiment_cloud_details,\n",
    "}\n",
    "\n",
    "if do_inference:\n",
    "    data.update({\"inference_dataset\": inference_dataset_id})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Batch Training Job\n",
    "\n",
    "Configure and initiate a batch training job on the DGX cloud, specifying the number of epochs and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"train#trainer#max_epochs\": 2,  # the key to override epochs\n",
    "}\n",
    "\n",
    "data = {\"name\": \"my_vista2d\", \"action\": \"train\", \"specs\": train_spec}\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx train job failed, got {response.json()}.\"\n",
    "train_job_id = response.json()\n",
    "print(\"Job creation succeeded with job ID: \", train_job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status and Downloading Job\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state. Here's what you need to know:\n",
    "\n",
    "1. **Basic Status Overview**: The monitoring functionality in our system is designed to inform you whether your jobs are in a pending, running, done, or error state. This status update allows you to quickly assess the overall progress and detect any immediate issues that may require attention.\n",
    "\n",
    "Status interpretation:\n",
    "- \"Pending\": MONAI cloud is looking for resources and preparing the datasets. This can take quite a while, and depends on the size of the dataset.\n",
    "- \"Running\": MONAI cloud has submitted the job to the DGX. \n",
    "- \"Done\": The training is complete\n",
    "- \"Error\": There is some error in the job. User probably wants to download the job as a `.tar.gz` archive and inspect the detailed log.\n",
    "\n",
    "2. **Detailed Logging Through Download API**: For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs, model checkpoints, and data outputs, which are instrumental for troubleshooting, in-depth analysis, and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(endpoint, headers, timeout=1800, interval=5, target_status=\"Done\"):\n",
    "    \"\"\"Helper function to wait for job to reach target status.\"\"\"\n",
    "    expected = [\"Pending\", \"Running\", \"Done\"]\n",
    "    assert target_status in expected, f\"Invalid target status: {target_status}\"\n",
    "    status_before_target = expected[:expected.index(target_status)]\n",
    "    start_time = time.time()\n",
    "    print(f\"Waiting for job to reach state {target_status} ...\")\n",
    "    status = None\n",
    "    while True:\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"\\nJob timeout after {timeout} seconds with last status {status_new}.\")\n",
    "            break\n",
    "        elif status_new not in status_before_target:\n",
    "            assert status_new == target_status, f\"Job failed with status: {status_new}\"\n",
    "            print(f\"\\nJob reached target status: {status_new}\")\n",
    "            break\n",
    "        print(f\"\\n{status_new}\", end=\"\", flush=True) if status_new != status else print(\".\", end=\"\", flush=True)\n",
    "        status = status_new\n",
    "        time.sleep(interval)\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Log Download\n",
    "\n",
    "Access and download job logs to troubleshoot or assess performance. The job log is available when the status of the job is `RUNNING`, `Error` or `Done`. This API is available for all kinds of jobs.\n",
    "\n",
    "Please note that the job log will not be immediately available after the status turns to `RUNNING` since it takes a while to prepare the environment for the running job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "status = response.json()[\"status\"].title()\n",
    "if status in [\"Running\", \"Done\", \"Error\"]:\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}/logs\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job logs, got {response.text}.\"\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Job status: {status}, logs are not available.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bundle Download\n",
    "\n",
    "Download the completed VISTA2d bundle once training is finished successfully. This API is only for the VISTA2d training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "# In order to download the job, the training process should be finished\n",
    "if response.json()[\"status\"] == \"Done\":\n",
    "    # Download the job log\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{train_job_id}:download_selective_files\"\n",
    "    endpoint += f\"?file_lists={train_job_id}/{vista_bundle_name}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to download trained bundle, got {response.json()}.\"\n",
    "    # Save to file\n",
    "    attachment_data = response.content\n",
    "    with open(f\"{train_job_id}.tar.gz\", 'wb') as f:\n",
    "        f.write(attachment_data)\n",
    "    print(f\"Trained bundle is downloaded as {train_job_id}.tar.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference (Optional)\n",
    "\n",
    "You can do the batch inference action inside an experiment.\n",
    "You need to run a batch training job first and set the training job id to the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_inference:    \n",
    "    inference_spec = {\"train_job_id\": train_job_id}\n",
    "    data = {\"name\": \"vista2d_infer\", \"action\": \"batchinfer\", \"specs\": inference_spec}\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "    assert response.status_code == 201, f\"Run batch inference job failed, got {response.json()}.\"\n",
    "    infer_job_id = response.json()\n",
    "    print(\"Job creation succeeded with job ID: \", infer_job_id)\n",
    "\n",
    "    # During the Job is Running \n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{infer_job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    for k, v in response.json().items():\n",
    "        if k != \"result\":\n",
    "            print(f\"{k}: {v}\")\n",
    "        else:\n",
    "            print(\"result:\")\n",
    "            for k1, v1 in v.items():\n",
    "                print(f\"    {k1}: {v1}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment and datasets to clean up resources once all jobs are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "for job in response.json()[\"jobs\"]:\n",
    "    job_id = job[\"id\"]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    # If the job is not done, need to cancel it first\n",
    "    if response.json()[\"status\"] != \"Done\":\n",
    "        endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "        response = requests.post(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Cancel job failed, got {response.json()}.\"\n",
    "        print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete train dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "# validation dataset\n",
    "endpoint = f\"{base_url}/datasets/{val_dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete val dataset failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "if do_inference:\n",
    "    # inference dataset\n",
    "    endpoint = f\"{base_url}/datasets/{inference_dataset_id}\"\n",
    "    response = requests.delete(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Delete inference dataset failed, got {response.json()}.\"\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching this pivotal milestone! With your dataset created and experiment selected, you're now fully equipped to leverage training features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
