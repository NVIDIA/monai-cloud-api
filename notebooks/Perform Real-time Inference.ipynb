{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Real-time Inference with NVIDIA Cloud APIs\n",
    "\n",
    "In this guide, we will guide you through the process of setting up a real-time inference system with MONAI cloud APIs. We will cover setting up the experiments, making on-the-fly predictions, and managing the outputs to ensure a seamless, efficient, and real-time decision-making pipeline.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Dataset Setup\n",
    "- Configuring Experiment to enable the real-time inference\n",
    "- Prepare the image ID for the inference request\n",
    "- Triggering Inference on a Specified Image\n",
    "- Stopping the experiment from Real-Time Inference mode\n",
    "- Cleaning up\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Transitioning to real-time inference can substantially elevate the responsiveness and applicability of AI models in healthcare. Analyzing and interpreting medical images as they are generated, and instantly providing insights, can be transformative, offering benefits such as improved patient outcomes and more efficient use of medical resources.\n",
    "\n",
    "## Dataset Setup\n",
    "\n",
    "We'll start by creating a new dataset for inference image sources. The dataset, hosted on a DICOMweb server, will be accessed using the `dicomweb` protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "# Dicom Server\n",
    "dicom_web_endpoint = \"<DICOMWeb address>\" # For example \"http://127.0.0.1:8042/dicom-web\".\n",
    "dicom_client_id = \"<DICOMWeb user ID>\"    # If Authentication is enabled, then provide username\n",
    "dicom_client_secret = \"<DICOMWeb secret>\" # If Authentication is enabled, then provide password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "print(response.status_code)\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{host_url}/api/v1/orgs/iasixjqzw1hj\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"mydataset\",\n",
    "    \"description\":\"a demo dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": f\"{dicom_web_endpoint}\",\n",
    "    \"client_id\": f\"{dicom_client_id}\",\n",
    "    \"client_secret\": f\"{dicom_client_secret}\",\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Experiment to Enable the Real-time Inference\n",
    "\n",
    "**Note:** We're going to use the `realtime_infer` parameter when creating our experiment as that will automatically load the experiment and make sure it's ready for real-time inference workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"List Base Experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "\n",
    "# VISTA-3D\n",
    "ptm_vista = [p for p in res if p[\"network_arch\"] == \"monai_vista3d\" and not len(p[\"base_experiment\"])][0][\"id\"]\n",
    "print(f\"Base Experiment ID for VISTA Experiment: {ptm_vista}\")\n",
    "    \n",
    "data = {\n",
    "  \"name\": \"my_vista\",\n",
    "  \"description\": \"based on vista\",\n",
    "  \"network_arch\": \"monai_vista3d\",\n",
    "  \"base_experiment\": [ ptm_vista ],\n",
    "  \"inference_dataset\": dataset_id,\n",
    "  \"eval_dataset\": dataset_id,\n",
    "  \"train_datasets\": [ dataset_id ],\n",
    "  \"realtime_infer\": True, # Auto loads MONAI bundle and enables real-time inference\n",
    "  \"model_params\":{\n",
    "      \"labels\": {\n",
    "            \"1\": \"liver\",\n",
    "            \"2\": \"kidney\",\n",
    "            \"3\": \"spleen\",\n",
    "            \"4\": \"pancreas\",\n",
    "            \"5\": \"right kidney\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "model_network = res[\"network_arch\"]\n",
    "print(\"Experiment creation succeeded with experiment ID:\", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the image ID for the inference request\n",
    "\n",
    "Getting the ID of the image to process:\n",
    "- The code sends a request to the \"nextimage\" action, instructing the system to automatically select and recommend the next image for processing.\n",
    "- This feature is particularly useful for workflows that require sequential processing of images or when the user prefers the system to determine the processing order. However, it's also designed to be flexible. \n",
    "- While this script uses the \"nextimage\" action to get an image ID automatically, users have the option to specify an image_id manually if they need to process a particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an inference image id with nextimage api\n",
    "data = {\n",
    "    \"action\": \"nextimage\"\n",
    "}\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Recommend image failed, got {response.json()}.\"\n",
    "res = response.json()\n",
    "inference_image_id = res[\"image\"]\n",
    "print(f\"Recommended Image to annotate: {inference_image_id}\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering Inference on a Specified Image\n",
    "\n",
    "Initiate an inference process on a particular image within an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"action\": \"inference\",\n",
    "    \"specs\": {\n",
    "        \"image\": inference_image_id,\n",
    "        \"bundle_params\": {\n",
    "            \"label_prompt\": list(range(1, 118))  # inference all 117 classes\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 201, f\"Run inference failed, got {response.json()}.\"\n",
    "print(\"Inference Successful.  Label is returned\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attachment_data = response.content\n",
    "tmp_save_path = \"pred.nrrd\"\n",
    "\n",
    "with open(tmp_save_path, 'wb') as f:\n",
    "    f.write(attachment_data)\n",
    "print(f\"Inference result downloaded to {tmp_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping the experiment from Real-Time Inference mode\n",
    "\n",
    "When the experiment is created with `realtime_infer` as `True`, it will reserve one GPU to process the inference requests.\n",
    "\n",
    "After we have finished the inference process, we would like to release the GPU resource for other tasks.\n",
    "\n",
    "To achieve this, we can switch the `realtime_infer` from `True` to `False`.\n",
    "\n",
    "Note: this step is irreversible, which means you can't set the `realtime_infer` from `False` to `True`. To bootstrap another inference, you will have to create another experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"realtime_infer\": False,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.patch(endpoint, json=data, headers=headers)\n",
    "assert response.status_code == 200, f\"stop job failed, got {response.json()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "Delete the experiment and dataset after jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showcases a streamlined approach to real-time inference, emphasizing automation in image selection and processing within a NVIDIA MONAI Cloud API-driven system. This method ensures efficient operations, allowing users to focus on model refinement and analysis while the system efficiently manages image selection and inference tasks, demonstrating the transformative potential of integrating advanced AI in real-time decision-making workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "az-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
