{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom MONAI Bundle on NVIDIA DGX Cloud\n",
    "\n",
    "This guide assists in training a custom MONAI Bundle on the NVIDIA DGX Cloud, focusing on using the cloud clusters' capabilities for medical imaging applications.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/monai-cloud-api/blob/main/notebooks/Training%20a%20Custom%20MONAI%20Bundle.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Setup\n",
    "- Dataset Creation\n",
    "- Custom MONAI Bundle Creation\n",
    "- Run a Batch Training Job\n",
    "- Monitoring Job Status and Logging\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Training a custom MONAI Bundle on NVIDIA DGX Cloud advances medical imaging projects. This guide ensures you utilize the cloud computing for deep learning effectively, from initializing to optimizing your MONAI bundle on DGX Cloud.\n",
    "\n",
    "### What You Can Expect to Learn\n",
    "\n",
    "In this guide, you will learn how to fully leverage the advanced computing power of NVIDIA DGX Cloud for training a custom MONAI Bundle tailored to your medical imaging needs. We will cover the entire process, from setting up your environment and creating a suitable dataset to running and monitoring batch training jobs effectively. By the end of this tutorial, you will have successfully trained a new MONAI Bundle using datasets stored on the remote cloud storage.\n",
    "\n",
    "If you have not generated your key or are unsure about the process, follow our step-by-step guide for [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import requests\" || pip install -q \"requests\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"https://api.monai.ngc.nvidia.com\"\n",
    "ngc_api_key = os.environ.get(\"MONAI_API_KEY\", \"<YOUR_API_KEY>\")  # we recommend using environment variables for API keys, but you can also hardcode them here\n",
    "\n",
    "# The cloud storage type used in this notebook. Currently only support `aws` and `azure`.\n",
    "cloud_type = \"azure\" # cloud storage provider: aws or azure\n",
    "cloud_account = \"account_name\" # if cloud_type == \"aws\"  should be \"access_key\"\n",
    "cloud_secret = \"access_key\" # if cloud_type == \"aws\" should be \"secret_key\"\n",
    "\n",
    "# Cloud storage credentials. Needed for storing the data and results of the experiments.\n",
    "access_id = \"<user name for the remote storage object>\"  # Please fill it with the actual Access ID\n",
    "access_secret = \"<secret for the remote storage object>\"  # Please fill it with the actual Access Secret\n",
    "\n",
    "# Dataset Cloud Storage URL. This is the cloud storage where the dataset is stored.\n",
    "container_url = \"<remote storage object address>\"\n",
    "\n",
    "# Experiment Cloud Storage. This is the storage where your jobs and experiments data will be stored.\n",
    "cs_bucket = \"<bucket or container name to push experiment job data to>\"  # Please fill it with the actual bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login into NGC and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "api_url = f\"{host_url}/api/v1\"\n",
    "response = requests.post(f\"{api_url}/login\", json={\"ngc_api_key\": ngc_api_key})\n",
    "response.raise_for_status()\n",
    "assert \"user_id\" in response.json(), \"user_id is not in response.\"\n",
    "assert \"token\" in response.json(), \"token is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "token = response.json()[\"token\"]\n",
    "\n",
    "# Construct the URL and Headers\n",
    "ngc_org = \"iasixjqzw1hj\"  # This is the default org for MONAI users. Please select the correct org if you are not using the default one.\n",
    "base_url = f\"{api_url}/orgs/{ngc_org}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "print(\"API Calls will be forwarded to\", base_url)\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow = False\n",
    "mlflow_server_address = \"\" # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\" # For example \"my_experiment\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Refer to [Training a MONAI Segmentation Bundle](./Training%20a%20MONAI%20Segmentation%20Bundle.ipynb) for creating a dataset on remote cloud storage. This tutorial simplifies by using the same dataset for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"MONAI_CLOUD\",\n",
    "    \"description\":\"Remote storage object dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": container_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MONAI Bundle Creation\n",
    "\n",
    "1. **MONAI Bundle**: Use the Spleen Segmentation MONAI bundle from the MONAI Model Zoo. Customize bundles to fit your applications.\n",
    "2. **Dataset Setup**: Use one dataset ID for this demo. Adjust according to your data structure.\n",
    "3. **Pretrained Weights**: Official MONAI bundles come with pretrained weights.\n",
    "\n",
    "Here are some notes about the payload used to create the experiment:\n",
    "\n",
    "- name: A user-defined name for the training experiment, here named \"my_spleen_seg\".\n",
    "- description: A brief description of the experiment. Optional\n",
    "- network_arch: Specifies the architecture of the network. The value \"monai_custom\" indicates that a custom network architecture is being used. The user must provide the `bundle_url` with such custom architecture.\n",
    "- train_datasets: A list of dataset IDs used for training the model. This payload supports only one dataset for the MONAI bundle, which is indicated by [ dataset_id ].\n",
    "- eval_dataset: The dataset ID used for evaluating the model. It can be different from the training dataset. Here, it's referred to as dataset_id.\n",
    "- bundle_url: Indicating the specific location of the MONAI bundle to be used in this training experiment.\n",
    "\n",
    "In this example, we use the same dataset for training and metrics validation. Users can also create two different datasets and use different dataset ids for `train_datasets` and `eval_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/monaihosting/spleen_ct_segmentation/versions/0.5.3/files/spleen_ct_segmentation_v0.5.3.zip\"\n",
    "\n",
    "experiment_cloud_details = {\n",
    "    \"cloud_type\": cloud_type,\n",
    "    \"cloud_file_type\": \"folder\",  # If the file is tar.gz key in \"file\", else \"folder\"\n",
    "    \"cloud_specific_details\": {\n",
    "        \"cloud_bucket_name\": cs_bucket,  # Bucket link to save files\n",
    "        cloud_account: access_id,  # Access and Secret for Azure\n",
    "        cloud_secret: access_secret,  # Access and Secret for Azure\n",
    "    }\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"name\": \"my_spleen_seg\",\n",
    "    \"description\": \"from MONAI model zoo\",\n",
    "    \"network_arch\": \"monai_custom\",  # must be using monai_custom\n",
    "    \"train_datasets\": [ dataset_id ],  # only one dataset is supported for MONAI bundle\n",
    "    \"eval_dataset\": dataset_id,  # it can be a different dataset\n",
    "    \"bundle_url\": bundle_url,\n",
    "    \"cloud_details\": experiment_cloud_details,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "base_experiment_ids = res[\"base_experiment\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Batch Training Job\n",
    "\n",
    "1. Users can submit jobs directly through our cloud API, modify the job submission payload to add specific parameters.\n",
    "1. Ensure payload format complies with the MONAI bundle configuration standards for streamlined integration.\n",
    "1. To enable a multi-gpu training, specfic `num_gpu` and modify the `config_file` accordingly. For example, for the `spleen_segmentation` bundle, you will need to include the `configs/multi_gpu_train.json` with `configs/train.json` (default config used for training) as below:\n",
    "\n",
    "    ```python\n",
    "    train_spec = {\n",
    "        ...\n",
    "        \"num_gpu\": 2,\n",
    "        \"config_file\": [\"configs/train.json\", \"configs/multi_gpu_train.json\"]\n",
    "    }\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "    \"epochs\": 2,\n",
    "}\n",
    "\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"save_execute_config\": False\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "  \"name\": \"my_spleen_seg\",\n",
    "  \"action\": \"train\",\n",
    "  \"specs\": train_spec\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx train job failed, got {response.json()}.\"\n",
    "\n",
    "job_id = response.json()\n",
    "print(f\"Job submitted successfully with {job_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initiating training with the MONAI bundle, the default configuration is set to utilize the train.json file located at configs/train.json. However, users have the flexibility to modify or override specific settings in this configuration file. This can be achieved by including key-value pairs within the training payload.\n",
    "\n",
    "**Customizing Configuration File in Payload**\n",
    "\n",
    "If your training scenario requires different or additional configuration files, you can specify this in the payload. For example, if the bundle relies on a different configuration file or multiple files, you can define them as follows:\n",
    "\n",
    "**Using an Alternate Single Configuration File**\n",
    "```json\n",
    "train_spec = {\n",
    "    ...\n",
    "    \"config_file\": \"configs/train_autoencoder.json\",\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the training will be based on the settings defined in `train_autoencoder.json` instead of the default train.json.\n",
    "\n",
    "**Specifying Multiple Configuration Files**\n",
    "```json\n",
    "train_spec = {\n",
    "    ...\n",
    "    \"config_file\": [\"configs/train.json\", \"configs/train_continual.json\"]\n",
    "}\n",
    "```\n",
    "Here, both `train.json` and `train_continual.json` are used, allowing for a more complex training setup that combines settings from multiple files.\n",
    "\n",
    "Important Notes\n",
    "- Adaptability: This method offers adaptability in training, catering to diverse and complex model training requirements.\n",
    "- Payload Customization: Carefully customize the payload to ensure that the training aligns with your specific model needs and dataset characteristics.\n",
    "- File Paths: Ensure that the file paths provided in the payload correctly point to the respective configuration files within the bundle structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Job Status and Logging\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(endpoint, headers, timeout=1800, interval=5, target_status=\"Done\"):\n",
    "    \"\"\"Helper function to wait for job to reach target status.\"\"\"\n",
    "    expected = [\"Pending\", \"Running\", \"Done\"]\n",
    "    assert target_status in expected, f\"Invalid target status: {target_status}\"\n",
    "    status_before_target = expected[:expected.index(target_status)]\n",
    "    start_time = time.time()\n",
    "    print(f\"Waiting for job to reach state {target_status} ...\")\n",
    "    status = None\n",
    "    while True:\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"\\nJob timeout after {timeout} seconds with last status {status_new}.\")\n",
    "            break\n",
    "        elif status_new not in status_before_target:\n",
    "            assert status_new == target_status, f\"Job failed with status: {status_new}\"\n",
    "            print(f\"\\nJob reached target status: {status_new}\")\n",
    "            break\n",
    "        print(f\"\\n{status_new}\", end=\"\", flush=True) if status_new != status else print(\".\", end=\"\", flush=True)\n",
    "        status = status_new\n",
    "        time.sleep(interval)\n",
    "\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, timeout=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Log Download\n",
    "\n",
    "Access and download job logs to troubleshoot or assess performance. The job log is available when the status of the job is `RUNNING`, `Error` or `Done`.\n",
    "\n",
    "Please note that the job log will not be immediately available after the status turns to `RUNNING` since it takes a while to prepare the environment for the running job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "status = response.json()[\"status\"].title()\n",
    "if status in [\"Running\", \"Done\", \"Error\"]:\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job logs, got {response.text}.\"\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Job status: {status}, logs are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the job results (checkpoint, scripts, logs, etc.)\n",
    "\n",
    "You'll find the results in the cloud storage bucket you specified when creating the experiment. The results will include the model checkpoints, scripts, logs, and other relevant data.\n",
    "\n",
    "The path to the results will be in the following format:\n",
    "\n",
    "```python\n",
    "f\"{bucket_name}/shared/orgs/{ngc_org}/users/{user_id}/jobs/{job_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment after all jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "# If the job is not done, need to cancel it first\n",
    "if response.json()[\"status\"] != \"Done\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Cancel job failed, got {response.json()}.\"\n",
    "    print(response)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If creating base experiments, also need to delete them before delete datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_experiment_id in base_experiment_ids:\n",
    "    endpoint = f\"{base_url}/experiments/{base_experiment_id}\"\n",
    "    response = requests.delete(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Delete base experiment failed, got {response.json()}.\"\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching this pivotal milestone! With your dataset created and experiment selected, you're now fully equipped to leverage the advanced customization features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
