{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom MONAI Bundle on NVIDIA DGX Cloud\n",
    "\n",
    "This comprehensive guide is designed to help you navigate the process of training a custom MONAI Bundle on the NVIDIA DGX Cloud, focusing on leveraging the powerful capabilities of DGX systems for medical imaging applications.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Dataset Creation\n",
    "- Custom MONAI Bundle Creation\n",
    "- Training on DGX Cloud\n",
    "- Monitoring and Downloading\n",
    "- Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Training a custom MONAI Bundle on the NVIDIA DGX Cloud represents a significant step in advancing medical imaging projects. This guide aims to facilitate your journey, ensuring you harness the full potential of DGX's high-performance computing for deep learning. We'll cover the steps from initializing your custom bundle to optimizing your training process on DGX Cloud.\n",
    "\n",
    "If you haven't already generated your key or if you're unsure about the process, follow our step-by-step for [Generating and Managing Your Credentials](./Generating%20and%20Managing%20Your%20Credentials.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Setup'></a>\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# API Endpoint and Credentials\n",
    "host_url = \"<MONAI Cloud API URL>\"\n",
    "ngc_api_key = \"<NGC API Key>\"\n",
    "# Object storage info\n",
    "container_url = \"<remote object storage address>\"\n",
    "access_id = \"<user id>\"\n",
    "access_secret = \"<storage secret>\"\n",
    "# training parameters\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "print(response.status_code)\n",
    "assert response.status_code == 201, f\"Login failed, got status code: {response.status_code}.\"\n",
    "assert \"user_id\" in response.json().keys(), \"user_id is not in response.\"\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys(), \"token is not in response.\"\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\", base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow = False\n",
    "mlflow_server_address = \"\" # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\" # For example \"my_experiment\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Please refer to [Training a MONAI Segmentation Bundle](./Training%20a%20MONAI%20Segmentation%20Bundle.ipynb) for how to create a dataset on remote cloud storage. For simplicity, this tutorial use the same dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Remote Object to Create Datasets\n",
    "\n",
    "After you've completed the steps above, it's time to run the API to create your dataset.  Below you'll find an example request along with associated parameters and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": \"MONAI_CLOUD\",\n",
    "    \"description\":\"Object storage dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": container_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create dataset failed, got {response.json()}.\"\n",
    "\n",
    "res = response.json()\n",
    "dataset_id = res[\"id\"]\n",
    "print(\"Dataset creation succeeded with dataset ID: \", dataset_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MONAI Bundle Creation\n",
    "\n",
    "1. **MONAI Bundle**: We're using the Spleen Segmentation bundle as an example. Choose the one fitting your application from the MONAI Model Zoo.\n",
    "2. **Dataset Setup**: All data is under one dataset ID for this demo. Adjust as per your data structure.\n",
    "3. **Pretrained Weights**: The Official MONAI bundles have pretrained weights.\n",
    "\n",
    "Here are some notes about the payload used to create the experiment:\n",
    "\n",
    "- name: A user-defined name for the training experiment, here named \"my_spleen_seg\".\n",
    "- description: A brief description of the experiment. Optional\n",
    "- network_arch: Specifies the architecture of the network. The value \"monai_custom\" indicates that a custom network architecture is being used. The user must provide the `bundle_url` with such custom architecture.\n",
    "- train_datasets: A list of dataset IDs used for training the model. This payload supports only one dataset for the MONAI bundle, which is indicated by [ dataset_id ].\n",
    "- eval_dataset: The dataset ID used for evaluating the model. It can be different from the training dataset. Here, it's referred to as dataset_id.\n",
    "- bundle_url: Indicating the specific location of the MONAI bundle to be used in this training experiment.\n",
    "\n",
    "In this example, we use the same dataset for training and metrics validation. Users can also create two different datasets and use different dataset ids for `train_datasets` and `eval_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/monaihosting/spleen_ct_segmentation/versions/0.5.3/files/spleen_ct_segmentation_v0.5.3.zip\"\n",
    "\n",
    "data = {\n",
    "  \"name\": \"my_spleen_seg\",\n",
    "  \"description\": \"from MONAI model zoo\",\n",
    "  \"network_arch\": \"monai_custom\",  # must be using monai_custom\n",
    "  \"train_datasets\": [ dataset_id ],  # only one dataset is supported for MONAI bundle\n",
    "  \"eval_dataset\": dataset_id,  # it can be a different dataset\n",
    "  \"bundle_url\": bundle_url,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Create experiment failed, got {response.json()}.\"\n",
    "\n",
    "res = response.json()\n",
    "experiment_id = res[\"id\"]\n",
    "base_experiment_ids = res[\"base_experiment\"]\n",
    "print(\"Experiment creation succeeded with experiment ID: \", experiment_id)\n",
    "print(\"---------------------------------\\n\")\n",
    "print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on DGX Cloud\n",
    "\n",
    "1. Users have the capability to submit jobs directly through our cloud API, enabling a streamlined and efficient process for initiating their projects.\n",
    "1. Additionally, users are empowered to modify the job submission payload, allowing the inclusion of additional parameters to tailor the execution according to specific requirements or preferences.\n",
    "1. The format of the payload aligns with the MONAI bundle configuration standards, ensuring a seamless integration and consistency in how data and parameters are structured and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = {\n",
    "  \"epochs\": epochs,\n",
    "}\n",
    "\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"train#handlers#-1#artifacts\": None\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "  \"action\": \"train\",\n",
    "  \"specs\": train_spec\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "assert response.status_code == 201, f\"Run dgx train job failed, got {response.json()}.\"\n",
    "\n",
    "job_id = response.json()\n",
    "print(f\"Job submitted successfully with {job_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initiating training with the MONAI bundle, the default configuration is set to utilize the train.json file located at configs/train.json. However, users have the flexibility to modify or override specific settings in this configuration file. This can be achieved by including key-value pairs within the training payload.\n",
    "\n",
    "**Customizing Configuration File in Payload**\n",
    "\n",
    "If your training scenario requires different or additional configuration files, you can specify this in the payload. For example, if the bundle relies on a different configuration file or multiple files, you can define them as follows:\n",
    "\n",
    "**Using an Alternate Single Configuration File**\n",
    "```json\n",
    "train_spec = {\n",
    "    ...\n",
    "    \"config_file\": \"configs/train_autoencoder.json\",\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the training will be based on the settings defined in `train_autoencoder.json` instead of the default train.json.\n",
    "\n",
    "**Specifying Multiple Configuration Files**\n",
    "```json\n",
    "train_spec = {\n",
    "    ...\n",
    "    \"config_file\": [\"configs/train.json\", \"configs/train_continual.json\"]\n",
    "}\n",
    "```\n",
    "Here, both `train.json` and `train_continual.json` are used, allowing for a more complex training setup that combines settings from multiple files.\n",
    "\n",
    "Important Notes\n",
    "- Adaptability: This method offers adaptability in training, catering to diverse and complex model training requirements.\n",
    "- Payload Customization: Carefully customize the payload to ensure that the training aligns with your specific model needs and dataset characteristics.\n",
    "- File Paths: Ensure that the file paths provided in the payload correctly point to the respective configuration files within the bundle structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Downloading\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively, especially for bundle customization. Here's what you need to know:\n",
    "\n",
    "1. **Basic Status Overview**: The monitoring functionality in our system is designed to inform you whether your jobs are in a pending, running, done, or error state. This status update allows you to quickly assess the overall progress and detect any immediate issues that may require attention.\n",
    "\n",
    "Status interpretation:\n",
    "- \"Pending\": MONAI cloud is looking for resources and preparing the datasets. This can take quite a while, and depends on the size of the dataset.\n",
    "- \"Running\": MONAI cloud has submitted the job to the DGX. \n",
    "- \"Done\": The training is complete\n",
    "- \"Error\": There is some error in the job. User probably wants to download the job as a `.tar.gz` archive and inspect the detailed log.\n",
    "\n",
    "2. **Detailed Logging Through Download API**: For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs, model checkpoints, and data outputs, which are instrumental for troubleshooting, in-depth analysis, and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running jobs\n",
    "def wait_for_job(endpoint, headers, timeout):\n",
    "    start_time = time.time()\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "    status = response.json()[\"status\"].title()\n",
    "    print(\"Waiting for job to complete...\")\n",
    "    print(status, end=\"\", flush=True)\n",
    "    while True:\n",
    "        if status not in [\"Pending\", \"Running\"]:\n",
    "            assert status == \"Done\", f\"Job failed with status: {status}\"\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "        status_new = response.json()[\"status\"].title()\n",
    "        if status_new != status:\n",
    "            status = status_new\n",
    "            print(f\"\\n{status}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        if time.time() - start_time > timeout:\n",
    "            assert False, f\"Job timeout after {timeout} seconds.\"\n",
    "    print(\"\\nJob completed successfully!\")\n",
    "\n",
    "# During the Job is Running \n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "assert response.status_code == 200, f\"Failed to get job status, got {response.json()}.\"\n",
    "for k, v in response.json().items():\n",
    "    if k != \"result\":\n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"result:\")\n",
    "        for k1, v1 in v.items():\n",
    "            print(f\"    {k1}: {v1}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "wait_for_job(endpoint, headers, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:download\"\n",
    "response = requests.get(endpoint, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert response.status_code == 200, f\"Failed to download job, got {response.json()}.\"\n",
    "\n",
    "# Save to file\n",
    "attachment_data = response.content\n",
    "with open(f\"{job_id}.tar.gz\", 'wb') as f:\n",
    "    f.write(attachment_data)\n",
    "print(f\"Bundle training results are downloaded as {job_id}.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the experiment after all jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete experiment failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If creating base experiments, also need to delete them before delete datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_experiment_id in base_experiment_ids:\n",
    "    endpoint = f\"{base_url}/experiments/{base_experiment_id}\"\n",
    "    response = requests.delete(endpoint, headers=headers)\n",
    "    assert response.status_code == 200, f\"Delete base experiment failed, got {response.json()}.\"\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete datasets after the experiment is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "endpoint = f\"{base_url}/datasets/{dataset_id}\"\n",
    "response = requests.delete(endpoint, headers=headers)\n",
    "assert response.status_code == 200, f\"Delete dataset failed, got {response.json()}.\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching this pivotal milestone! With your dataset created and experiment selected, you're now fully equipped to leverage the advanced customization features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
