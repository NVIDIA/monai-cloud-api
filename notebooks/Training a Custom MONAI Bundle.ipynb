{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# API Endpoint and Credentials\n",
    "host_url = \"<MONAI Cloud API URL>\"\n",
    "ngc_api_key = \"<NGC API Key>\"\n",
    "\n",
    "# NGC UID \n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"user_id\" in response.json().keys()\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Construct the URL and Headers\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# MLFlow server\n",
    "use_mlflow =False\n",
    "mlflow_server_address = \"\" # For example \"http://127.0.0.1:5000\".\n",
    "mlflow_experiment_name = \"\" # For example \"my_experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "### **1. Remote Object as Data Sources**\n",
    "\n",
    "MONAI Cloud platform supports a range of other cloud storage solutions, including Azure Blob Storage, Google Cloud Storage (GCP) and Amazon S3, providing you with the flexibility to choose the service that best fits your project's needs. Below is an example of Azure:\n",
    "\n",
    "**Steps:**\n",
    "1. Creating a Storage Account and Container\n",
    "   - **Storage Account**: Start by creating a new storage account in your Azure portal. This account will host your blob storage containers.\n",
    "   - **Container Creation**: Within your storage account, create a new container. This container will hold your datasets.\n",
    "\n",
    "2. Container URL\n",
    "   - Once the container is created, you will be provided with a unique URL that can be used to access it. This URL will be essential for accessing your data.\n",
    "\n",
    "## Obtaining Credentials\n",
    "\n",
    "- **Access Keys**: Access your storage account and navigate to the 'Access keys' section. Here, you will find the necessary credentials to access your Blob Storage programmatically.\n",
    "- **Shared Access Signature (SAS)**: Alternatively, you can create a SAS for more granular control over permissions and access duration.\n",
    "\n",
    "## Creating a Manifest JSON File\n",
    "\n",
    "In the root of your Azure container, create a manifest JSON file to keep track of your datasets. The file format is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"root_path\": \"https://[your-storage-account-name].blob.core.windows.net/[your-container-name]\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"path\": [\"path/to/your/image_1\"],\n",
    "                \"id\": \"unique-uuid-1\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"path\": [\"path/to/your/label_1\"],\n",
    "                \"id\": \"unique-uuid-2\"\n",
    "            }\n",
    "        },\n",
    "        // Additional data objects follow the same format\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "- Each dataset (training, testing, etc.) should have their own root directory\n",
    "- All the data should be under a root directory\n",
    "- The root directory should contain a `manifest.json` file\n",
    "- The `manifest.json` file should contain \"data\" field, which is a list of all the data entries\n",
    "- Each data entry should contain \"image\" and \"label\" fields\n",
    "- Each \"image\"/\"label\" field should contain \"path\" field, which is the list of relative path to the image/label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_url = \"<remote object storage address>\"\n",
    "access_id = \"<user id>\"\n",
    "access_secret = \"<storage secret>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Remote Object to Create Datasets\n",
    "\n",
    "After you've completed the steps above, it's time to run the API to create your dataset.  Below you'll find an example request along with associated parameters and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation succeeded with dataset ID：  50942089-f445-466d-b42d-d9a0a0ed53c0\n",
      "---------------------------------\n",
      "\n",
      "{\n",
      "  \"actions\": [\n",
      "    \"nextimage\",\n",
      "    \"cacheimage\",\n",
      "    \"notify\"\n",
      "  ],\n",
      "  \"client_url\": \"https://monaiserviceadmin.blob.core.windows.net/msd-spleen-subset\",\n",
      "  \"created_on\": \"2024-01-07T15:21:40.338948\",\n",
      "  \"description\": \"Object storage dataset\",\n",
      "  \"docker_env_vars\": {},\n",
      "  \"format\": \"monai\",\n",
      "  \"id\": \"50942089-f445-466d-b42d-d9a0a0ed53c0\",\n",
      "  \"jobs\": [],\n",
      "  \"last_modified\": \"2024-01-07T15:21:40.338959\",\n",
      "  \"logo\": \"https://www.nvidia.com\",\n",
      "  \"name\": \"MONAI_CLOUD\",\n",
      "  \"pull\": null,\n",
      "  \"type\": \"semantic_segmentation\",\n",
      "  \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"name\": \"MONAI_CLOUD\",\n",
    "    \"description\":\"Object storage dataset\",\n",
    "    \"type\": \"semantic_segmentation\",\n",
    "    \"format\": \"monai\",\n",
    "    \"client_url\": container_url,\n",
    "    \"client_id\": access_id,\n",
    "    \"client_secret\": access_secret,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    res = response.json()\n",
    "    dataset_id = res[\"id\"]\n",
    "    print(\"Dataset creation succeeded with dataset ID： \", dataset_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MONAI Bundle Creation\n",
    "\n",
    "1. **MONAI Bundle**: We're using the Spleen Segmentation bundle as an example. Choose the one fitting your application from the MONAI Model Zoo.\n",
    "2. **Dataset Setup**: All data is under one dataset ID for this demo. Adjust as per your data structure.\n",
    "3. **Pretrained Weights**: The Official MONAI bundles have pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation succeeded with model ID：  ff3699b9-2965-403e-a532-7a7876525f1e\n",
      "---------------------------------\n",
      "\n",
      "{\n",
      "  \"actions\": [\n",
      "    \"train\"\n",
      "  ],\n",
      "  \"additional_id_info\": null,\n",
      "  \"automl_add_hyperparameters\": \"[]\",\n",
      "  \"automl_algorithm\": null,\n",
      "  \"automl_enabled\": false,\n",
      "  \"automl_remove_hyperparameters\": \"[]\",\n",
      "  \"base_experiment\": [\n",
      "    \"708809fe-2a0b-4a06-943c-53f6717b5483\"\n",
      "  ],\n",
      "  \"calibration_dataset\": null,\n",
      "  \"checkpoint_choose_method\": \"best_model\",\n",
      "  \"checkpoint_epoch_number\": {},\n",
      "  \"created_on\": \"2024-01-07T15:21:44.155081\",\n",
      "  \"dataset_type\": \"user_custom\",\n",
      "  \"description\": \"from MONAI model zoo\",\n",
      "  \"docker_env_vars\": {},\n",
      "  \"encryption_key\": \"tlt_encode\",\n",
      "  \"eval_dataset\": \"50942089-f445-466d-b42d-d9a0a0ed53c0\",\n",
      "  \"id\": \"ff3699b9-2965-403e-a532-7a7876525f1e\",\n",
      "  \"inference_dataset\": null,\n",
      "  \"is_ptm_backbone\": true,\n",
      "  \"jobs\": [],\n",
      "  \"last_modified\": \"2024-01-07T15:21:44.155092\",\n",
      "  \"logo\": \"https://www.nvidia.com\",\n",
      "  \"metric\": null,\n",
      "  \"model_params\": {},\n",
      "  \"name\": \"my_spleen_seg\",\n",
      "  \"network_arch\": \"monai_custom\",\n",
      "  \"ngc_path\": \"\",\n",
      "  \"public\": false,\n",
      "  \"read_only\": false,\n",
      "  \"realtime_infer\": false,\n",
      "  \"realtime_infer_request_timeout\": 60,\n",
      "  \"realtime_infer_support\": false,\n",
      "  \"train_datasets\": [\n",
      "    \"50942089-f445-466d-b42d-d9a0a0ed53c0\"\n",
      "  ],\n",
      "  \"type\": \"medical\",\n",
      "  \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bundle_url = \"https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/spleen_ct_segmentation_v0.5.3.zip\"\n",
    "\n",
    "data = {\n",
    "  \"name\": \"my_spleen_seg\",\n",
    "  \"description\": \"from MONAI model zoo\",\n",
    "  \"network_arch\": \"monai_custom\",  # must be using monai_custom\n",
    "  \"eval_dataset\": dataset_id,\n",
    "  \"train_datasets\": [ dataset_id ],\n",
    "  \"bundle_url\": bundle_url,\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "if response.status_code == 201:\n",
    "    res = response.json()\n",
    "    experiment_id = res[\"id\"]\n",
    "    print(\"Model creation succeeded with model ID： \", experiment_id)\n",
    "    print(\"---------------------------------\\n\")\n",
    "    print(json.dumps(res, indent=2))\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on DGX Cloud\n",
    "\n",
    "1. Users have the capability to submit jobs directly through our cloud API, enabling a streamlined and efficient process for initiating their projects.\n",
    "1. Additionally, users are empowered to modify the job submission payload, allowing the inclusion of additional parameters to tailor the execution according to specific requirements or preferences.\n",
    "1. The format of the payload aligns with the MONAI bundle configuration standards, ensuring a seamless integration and consistency in how data and parameters are structured and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully with a8a08170-155b-47b0-81f4-c9713842510e.\n"
     ]
    }
   ],
   "source": [
    "train_spec = {\n",
    "    \"epochs\": 2,\n",
    "  }\n",
    "if use_mlflow:\n",
    "    mlflow_spec = {\n",
    "        \"tracking\": \"mlflow\",\n",
    "        \"tracking_uri\": f\"{mlflow_server_address}\",\n",
    "        \"experiment_name\": f\"{mlflow_experiment_name}\",\n",
    "        \"train#handlers#-1#artifacts\": None\n",
    "    }\n",
    "    train_spec.update(mlflow_spec)\n",
    "\n",
    "data = {\n",
    "  \"action\": \"train\",\n",
    "  \"specs\": train_spec\n",
    "}\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    job_id = response.json()\n",
    "    print(f\"Job submitted successfully with {job_id}.\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Downloading\n",
    "\n",
    "Monitoring the status of your jobs is a crucial aspect of managing workflows effectively. In our system, the job monitoring feature provides a straightforward yet essential overview of your job's current state. Here's what you need to know:\n",
    "\n",
    "1. **Basic Status Overview**: The monitoring functionality in our system is designed to inform you whether your jobs are in a pending, running, done, or error state. This status update allows you to quickly assess the overall progress and detect any immediate issues that may require attention.\n",
    "\n",
    "Status interpretation:\n",
    "- \"Pending\": MONAI cloud is looking for resources and preparing the datasets. This can take quite a while, and depends on the size of the dataset.\n",
    "- \"Running\": MONAI cloud has submitted the job to the DGX. \n",
    "- \"Done\": The training is complete\n",
    "- \"Error\": There is some error in the job. User probably wants to download the job as a `.tar.gz` archive and inspect the detailed log.\n",
    "\n",
    "2. **Detailed Logging Through Download API**: For a more comprehensive view and detailed logging of your jobs, our platform offers a Download API. This API enables you to access in-depth logs, model checkpoints, and data outputs, which are instrumental for troubleshooting, in-depth analysis, and gaining insights into the specifics of your job's execution. The Download API is particularly useful if your job encounters an error or if you need to understand the performance and behavior of your job in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    for k, v in response.json().items():\n",
    "        if k != \"result\":\n",
    "            print(f\"{k}: {v}\")\n",
    "        else:\n",
    "            print(\"result:\")\n",
    "            for k1, v1 in v.items():\n",
    "                print(f\"    {k1}: {v1}\")\n",
    "else:\n",
    "    print(response.json())\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/download\"\n",
    "response = requests.get(endpoint, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle training results are downloaded as ce111b2c-c1d9-4fcd-85d6-c402df4484d7.tar.gz\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    #save to file\n",
    "    attachment_data = response.content\n",
    "    with open(f\"{job_id}.tar.gz\", 'wb') as f:\n",
    "        f.write(attachment_data)\n",
    "    print(f\"Bundle training results are downloaded as {job_id}.tar.gz\")\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\"Congratulations on reaching this pivotal milestone! With your dataset created and model selected, you're now fully equipped to leverage the advanced features of the NVIDIA MONAI Cloud APIs for your medical imaging projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
